{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e86681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59a416cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AAPL.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_30492\\1014810567.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mdf\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"AAPL.csv\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mparse_dates\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"Date\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mindex_col\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfreq\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mmissing_dates\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdate_range\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstart\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mend\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmax\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdifference\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmissing_dates\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Application\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    910\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    911\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 912\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    913\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    914\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Application\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    575\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    576\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 577\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    578\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    579\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Application\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1405\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1406\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[1;33m|\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1407\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1408\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1409\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Application\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1659\u001B[0m                 \u001B[1;32mif\u001B[0m \u001B[1;34m\"b\"\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1660\u001B[0m                     \u001B[0mmode\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;34m\"b\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1661\u001B[1;33m             self.handles = get_handle(\n\u001B[0m\u001B[0;32m   1662\u001B[0m                 \u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1663\u001B[0m                 \u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Application\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    857\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;34m\"b\"\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    858\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 859\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    860\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    861\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'AAPL.csv'"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"AAPL.csv\",parse_dates=[\"Date\"],index_col=[0])\n",
    "print(df.index.freq)\n",
    "missing_dates = pd.date_range(start=df.index.min(), end=df.index.max()).difference(df.index)\n",
    "print(missing_dates)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe6f18e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-07-01</th>\n",
       "      <td>14.381786</td>\n",
       "      <td>14.723929</td>\n",
       "      <td>14.329286</td>\n",
       "      <td>14.615000</td>\n",
       "      <td>12.688703</td>\n",
       "      <td>391053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-02</th>\n",
       "      <td>14.641429</td>\n",
       "      <td>15.058214</td>\n",
       "      <td>14.623929</td>\n",
       "      <td>14.946071</td>\n",
       "      <td>12.976134</td>\n",
       "      <td>469865200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-03</th>\n",
       "      <td>15.030714</td>\n",
       "      <td>15.106429</td>\n",
       "      <td>14.908929</td>\n",
       "      <td>15.028571</td>\n",
       "      <td>13.047762</td>\n",
       "      <td>240928800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-05</th>\n",
       "      <td>15.013929</td>\n",
       "      <td>15.117500</td>\n",
       "      <td>14.833929</td>\n",
       "      <td>14.907857</td>\n",
       "      <td>12.942959</td>\n",
       "      <td>274024800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-07-08</th>\n",
       "      <td>15.003929</td>\n",
       "      <td>15.035714</td>\n",
       "      <td>14.666071</td>\n",
       "      <td>14.823214</td>\n",
       "      <td>12.869470</td>\n",
       "      <td>298138400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Open       High        Low      Close  Adj Close     Volume\n",
       "Date                                                                        \n",
       "2013-07-01  14.381786  14.723929  14.329286  14.615000  12.688703  391053600\n",
       "2013-07-02  14.641429  15.058214  14.623929  14.946071  12.976134  469865200\n",
       "2013-07-03  15.030714  15.106429  14.908929  15.028571  13.047762  240928800\n",
       "2013-07-05  15.013929  15.117500  14.833929  14.907857  12.942959  274024800\n",
       "2013-07-08  15.003929  15.035714  14.666071  14.823214  12.869470  298138400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5babdf94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-06-26</th>\n",
       "      <td>186.830002</td>\n",
       "      <td>188.050003</td>\n",
       "      <td>185.229996</td>\n",
       "      <td>185.270004</td>\n",
       "      <td>185.270004</td>\n",
       "      <td>48088700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-27</th>\n",
       "      <td>185.889999</td>\n",
       "      <td>188.389999</td>\n",
       "      <td>185.669998</td>\n",
       "      <td>188.059998</td>\n",
       "      <td>188.059998</td>\n",
       "      <td>50730800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-28</th>\n",
       "      <td>187.929993</td>\n",
       "      <td>189.899994</td>\n",
       "      <td>187.600006</td>\n",
       "      <td>189.250000</td>\n",
       "      <td>189.250000</td>\n",
       "      <td>51216800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-29</th>\n",
       "      <td>189.080002</td>\n",
       "      <td>190.070007</td>\n",
       "      <td>188.940002</td>\n",
       "      <td>189.589996</td>\n",
       "      <td>189.589996</td>\n",
       "      <td>46347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-30</th>\n",
       "      <td>191.630005</td>\n",
       "      <td>194.479996</td>\n",
       "      <td>191.259995</td>\n",
       "      <td>193.970001</td>\n",
       "      <td>193.970001</td>\n",
       "      <td>85069600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close   \n",
       "Date                                                                     \n",
       "2023-06-26  186.830002  188.050003  185.229996  185.270004  185.270004  \\\n",
       "2023-06-27  185.889999  188.389999  185.669998  188.059998  188.059998   \n",
       "2023-06-28  187.929993  189.899994  187.600006  189.250000  189.250000   \n",
       "2023-06-29  189.080002  190.070007  188.940002  189.589996  189.589996   \n",
       "2023-06-30  191.630005  194.479996  191.259995  193.970001  193.970001   \n",
       "\n",
       "              Volume  \n",
       "Date                  \n",
       "2023-06-26  48088700  \n",
       "2023-06-27  50730800  \n",
       "2023-06-28  51216800  \n",
       "2023-06-29  46347300  \n",
       "2023-06-30  85069600  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c028deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(data, prediction_size):\n",
    "    train_size = int(len(data) * (1-prediction_size))\n",
    "    train = pd.DataFrame(data[0:train_size])\n",
    "    test = pd.DataFrame(data[train_size:len(data)])\n",
    "    return train, test\n",
    "df_train, df_test = train_test_split(df, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0b22b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2014, 6)\n",
      "(504, 6)\n"
     ]
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a86012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "df_train_scaled = scaler.fit_transform(df_train)\n",
    "df_test_scaled = scaler.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d33a6b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.33880404],\n",
       "       [0.00200934, 0.0025642 , 0.00231928, 0.00257553, 0.00223922,\n",
       "        0.41606462],\n",
       "       [0.00502195, 0.00293405, 0.00456266, 0.00321732, 0.00279723,\n",
       "        0.19163371],\n",
       "       ...,\n",
       "       [0.9215281 , 0.9140881 , 0.93262005, 0.92181724, 0.92612438,\n",
       "        0.02483709],\n",
       "       [0.92114114, 0.92452027, 0.93687072, 0.93480878, 0.93898379,\n",
       "        0.01633536],\n",
       "       [0.93189813, 0.93403198, 0.94474221, 0.94686684, 0.95091897,\n",
       "        0.01873205]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462056c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.42500307e-01, 9.41089026e-01, 9.56706791e-01, 9.51767888e-01,\n",
       "        9.55770295e-01, 1.74628302e-02],\n",
       "       [9.45828073e-01, 9.40475354e-01, 9.55840927e-01, 9.54179472e-01,\n",
       "        9.58157373e-01, 6.89929527e-03],\n",
       "       [9.55888481e-01, 9.60956128e-01, 9.71505236e-01, 9.75106018e-01,\n",
       "        9.78870838e-01, 3.27471935e-02],\n",
       "       ...,\n",
       "       [1.34306298e+00, 1.34372440e+00, 1.36389919e+00, 1.35855144e+00,\n",
       "        1.37549128e+00, 5.65526934e-03],\n",
       "       [1.35196272e+00, 1.34502852e+00, 1.37444696e+00, 1.36119639e+00,\n",
       "        1.37814000e+00, 8.81601670e-04],\n",
       "       [1.37169680e+00, 1.37885626e+00, 1.39270877e+00, 1.39527010e+00,\n",
       "        1.41226219e+00, 3.88418422e-02]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6a58026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def createXY(dataset,n_past):\n",
    "#     dataX = []\n",
    "#     dataY = []\n",
    "#     for i in range(n_past, len(dataset)):\n",
    "#             dataX.append(dataset[i - n_past:i, 0:dataset.shape[1]])\n",
    "#             dataY.append(dataset[i,0])\n",
    "#     return np.array(dataX),np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a7181c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series_data(input_data, history_length):\n",
    "    X_data, Y_data = [], []\n",
    "\n",
    "    for idx in range(history_length, len(input_data)):\n",
    "        X_data.append(input_data[idx-history_length :idx, 0:input_data.shape[1]])\n",
    "        Y_data.append(input_data[idx, 0]) #这里，对于每个 idx，我们只取第一个特征的值作为输出。这意味着我们正在尝试预测每个时间点的第一个特征的值\n",
    "\n",
    "    return np.array(X_data), np.array(Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06fa55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#假设你只想预测第1、第3和第5个特征（记住，索引是从0开始的）\n",
    "#Y_data.append(input_data[idx, [0, 2, 4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "487609c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_scaled.shape[1] #总共6列值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18b62d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX,trainY=generate_time_series_data(df_train_scaled,30)\n",
    "testX,testY=generate_time_series_data(df_test_scaled,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdaf6a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX Shape--  (1984, 30, 6)\n",
      "trainY Shape--  (1984,)\n"
     ]
    }
   ],
   "source": [
    "print(\"trainX Shape-- \",trainX.shape) #trainX consists of 1984 time Windows, each consisting of 30 consecutive time steps with 6 features per time step\n",
    "print(\"trainY Shape-- \",trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c78e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testX Shape--  (474, 30, 6)\n",
      "testY Shape--  (474,)\n"
     ]
    }
   ],
   "source": [
    "print(\"testX Shape-- \",testX.shape)\n",
    "print(\"testY Shape-- \",testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbf9d2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainX[0]-- \n",
      " [[0.         0.         0.         0.         0.         0.33880404]\n",
      " [0.00200934 0.0025642  0.00231928 0.00257553 0.00223922 0.41606462]\n",
      " [0.00502195 0.00293405 0.00456266 0.00321732 0.00279723 0.19163371]\n",
      " [0.00489206 0.00301897 0.00397229 0.00227824 0.00198077 0.22407838]\n",
      " [0.00481467 0.00239161 0.002651   0.00161978 0.00140826 0.24771742]\n",
      " [0.00301539 0.0030765  0.0025751  0.00364798 0.00317164 0.30109192]\n",
      " [0.00467371 0.00343264 0.00478755 0.00319788 0.00278032 0.23131393]\n",
      " [0.00559961 0.00437778 0.00560844 0.00502048 0.00436496 0.27531735]\n",
      " [0.00689862 0.00479967 0.00623816 0.00480377 0.00417651 0.22950779]\n",
      " [0.00616897 0.00525717 0.00662892 0.00506215 0.00440116 0.19260266]\n",
      " [0.00658631 0.00505171 0.00645181 0.00582898 0.0050679  0.16772293]\n",
      " [0.00746522 0.00546538 0.00759036 0.00585954 0.00509445 0.15052067]\n",
      " [0.00848233 0.00619135 0.00826226 0.0062624  0.00544472 0.17001766]\n",
      " [0.00840494 0.00594754 0.00650241 0.00437034 0.0037997  0.21887955]\n",
      " [0.00739889 0.00478871 0.00681727 0.0047482  0.00412821 0.15915336]\n",
      " [0.00644259 0.00402438 0.00491687 0.00271445 0.00236001 0.31757227]\n",
      " [0.01001627 0.00885418 0.00956948 0.00869345 0.00755835 0.53573384]\n",
      " [0.01050548 0.00798027 0.0097241  0.008135   0.00707282 0.18042356]\n",
      " [0.00901299 0.00788165 0.00931085 0.00882682 0.00767429 0.1516598 ]\n",
      " [0.01053312 0.01033354 0.01095824 0.01071609 0.00931688 0.19862222]\n",
      " [0.01306483 0.01229505 0.01349679 0.01225253 0.01065273 0.25877935]\n",
      " [0.01445506 0.01234709 0.01355302 0.01203302 0.01046188 0.27204818]\n",
      " [0.01466512 0.01219916 0.01462973 0.01318604 0.01146435 0.15763818]\n",
      " [0.01528975 0.01385658 0.01558556 0.01481415 0.01287989 0.22482225]\n",
      " [0.01713601 0.0159989  0.01712893 0.01673399 0.01454904 0.26802691]\n",
      " [0.01805639 0.01633312 0.01713455 0.01556709 0.0135345  0.28371399]\n",
      " [0.01689003 0.0149935  0.01702209 0.01549207 0.01346928 0.24842286]\n",
      " [0.01690662 0.01419903 0.01594819 0.01438907 0.0132456  0.2061899 ]\n",
      " [0.01546386 0.01320183 0.01473937 0.01256646 0.0116505  0.21705968]\n",
      " [0.01497191 0.01544551 0.01557712 0.01615332 0.01478964 0.31270832]]\n",
      "\n",
      "trainY[0]--  0.018863438917514894\n"
     ]
    }
   ],
   "source": [
    "print(\"trainX[0]-- \\n\",trainX[0])\n",
    "print(\"\\ntrainY[0]-- \",trainY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d165e52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42ffe2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eq220\\AppData\\Local\\Temp\\ipykernel_10680\\3563693491.py:11: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  grid_model = KerasRegressor(build_fn=build_model,verbose=1,validation_data=(testX,testY))\n"
     ]
    }
   ],
   "source": [
    "def build_model(optimizer):\n",
    "    grid_model = Sequential()\n",
    "    grid_model.add(LSTM(50,return_sequences=True,input_shape=(30,6)))\n",
    "    grid_model.add(LSTM(50))\n",
    "    grid_model.add(Dropout(0.2))\n",
    "    grid_model.add(Dense(1))\n",
    "\n",
    "    grid_model.compile(loss = 'mse',optimizer = optimizer)\n",
    "    return grid_model\n",
    "\n",
    "grid_model = KerasRegressor(build_fn=build_model,verbose=1,validation_data=(testX,testY))\n",
    "parameters = {'batch_size' : [ 32, 64], # , 128, 256, 512\n",
    "              'epochs' : [ 50, 100], # , 200\n",
    "              'optimizer' : ['adam','Adadelta'] }\n",
    "\n",
    "grid_search  = GridSearchCV(estimator = grid_model,\n",
    "                            param_grid = parameters,\n",
    "                            cv = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dff190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# 定义超参数空间\n",
    "space  = [Integer(32, 256, name='lstm_neurons'),\n",
    "          Real(0.1, 0.9, name='dropout_rate'),\n",
    "          Real(1e-4, 1e-2, name='learning_rate', prior='log-uniform')]\n",
    "\n",
    "# 定义优化目标函数\n",
    "@use_named_args(space)\n",
    "def objective(**params):\n",
    "    # 在这里构建您的 LSTM 模型\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(params['lstm_neurons'], input_shape=(trainX.shape[1], trainX.shape[2])))\n",
    "    model.add(Dropout(params['dropout_rate']))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=params['learning_rate'])\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    \n",
    "    history = model.fit(trainX, trainY, epochs=50, batch_size=32, validation_data=(testX, testY), verbose=0, shuffle=False)\n",
    "    \n",
    "    # 返回验证损失\n",
    "    return history.history['val_loss'][-1]\n",
    "\n",
    "# 执行贝叶斯优化\n",
    "res = gp_minimize(objective, space, n_calls=50, random_state=0)\n",
    "\n",
    "# 输出最佳参数\n",
    "print(\"Best parameters: {}\".format(res.x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fc54625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "31/31 [==============================] - 7s 74ms/step - loss: 0.0237 - val_loss: 0.0062\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0034 - val_loss: 0.0047\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0028 - val_loss: 0.0035\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0026 - val_loss: 0.0069\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0022 - val_loss: 0.0044\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0020 - val_loss: 0.0054\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0020 - val_loss: 0.0036\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0021 - val_loss: 0.0025\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.0018 - val_loss: 0.0056\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0021 - val_loss: 0.0019\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0016 - val_loss: 0.0036\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0017 - val_loss: 0.0133\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0019 - val_loss: 0.0051\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0019 - val_loss: 0.0033\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0018 - val_loss: 0.0030\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0018 - val_loss: 0.0016\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0017 - val_loss: 0.0057\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0017 - val_loss: 0.0015\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 0.0019 - val_loss: 0.0014\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0015 - val_loss: 0.0019\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0016 - val_loss: 0.0054\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 0.0018 - val_loss: 0.0154\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0021 - val_loss: 0.0017\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0016 - val_loss: 0.0020\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0018 - val_loss: 0.0039\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0017 - val_loss: 0.0028\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0014 - val_loss: 0.0045\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0017 - val_loss: 0.0020\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0014 - val_loss: 0.0059\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 2s 54ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.0015 - val_loss: 0.0027\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0013 - val_loss: 0.0019\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0015 - val_loss: 0.0022\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0015 - val_loss: 0.0034\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0018 - val_loss: 0.0011\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0012 - val_loss: 0.0026\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0012 - val_loss: 0.0015\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0012 - val_loss: 0.0017\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 1.8673e-04\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 7s 77ms/step - loss: 0.0011 - val_loss: 0.2485\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 2.1523e-04 - val_loss: 0.1619\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 1.4473e-04 - val_loss: 0.1210\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 1.4892e-04 - val_loss: 0.0795\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 1.2207e-04 - val_loss: 0.0586\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 1.2599e-04 - val_loss: 0.0678\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.0486e-04 - val_loss: 0.0407\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 1.0878e-04 - val_loss: 0.0343\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 9.9095e-05 - val_loss: 0.0344\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 8.7274e-05 - val_loss: 0.0431\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 8.4809e-05 - val_loss: 0.0364\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 7.9721e-05 - val_loss: 0.0311\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 8.6743e-05 - val_loss: 0.0387\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 8.1526e-05 - val_loss: 0.0336\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 8.1235e-05 - val_loss: 0.0206\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 7.7880e-05 - val_loss: 0.0211\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 7.2167e-05 - val_loss: 0.0194\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 6.7248e-05 - val_loss: 0.0115\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 5.6322e-05 - val_loss: 0.0159\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 6.1372e-05 - val_loss: 0.0132\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 6.7728e-05 - val_loss: 0.0087\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 6.2953e-05 - val_loss: 0.0133\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 6.2937e-05 - val_loss: 0.0109\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 5.6631e-05 - val_loss: 0.0133\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 6.8378e-05 - val_loss: 0.0158\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 5.6700e-05 - val_loss: 0.0062\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 5.9300e-05 - val_loss: 0.0073\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 5.9845e-05 - val_loss: 0.0072\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 5.6873e-05 - val_loss: 0.0097\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 5.4486e-05 - val_loss: 0.0064\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 4.9441e-05 - val_loss: 0.0029\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 4.9647e-05 - val_loss: 0.0019\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 4.7530e-05 - val_loss: 0.0043\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 4.4839e-05 - val_loss: 0.0031\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 4.5065e-05 - val_loss: 0.0023\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 4.8767e-05 - val_loss: 0.0049\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 4.4406e-05 - val_loss: 0.0069\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 4.2271e-05 - val_loss: 0.0050\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 3.9788e-05 - val_loss: 0.0060\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 4.2828e-05 - val_loss: 0.0056\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 3.8490e-05 - val_loss: 0.0032\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 3.8911e-05 - val_loss: 0.0035\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 2s 55ms/step - loss: 3.5139e-05 - val_loss: 0.0023\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 3.7329e-05 - val_loss: 0.0014\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 3.7883e-05 - val_loss: 0.0011\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 3.6999e-05 - val_loss: 0.0011\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 3.2009e-05 - val_loss: 9.1181e-04\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 3.7257e-05 - val_loss: 8.3002e-04\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 3.7380e-05 - val_loss: 0.0010\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 3.8119e-05 - val_loss: 0.0023\n",
      "31/31 [==============================] - 1s 23ms/step - loss: 4.7424e-04\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 6s 72ms/step - loss: 0.1926 - val_loss: 0.9355\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.1896 - val_loss: 0.9182\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.1858 - val_loss: 0.9009\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 1s 24ms/step - loss: 0.1822 - val_loss: 0.8833\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.1771 - val_loss: 0.8658\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.1733 - val_loss: 0.8480\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.1676 - val_loss: 0.8303\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.1646 - val_loss: 0.8126\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 2s 47ms/step - loss: 0.1603 - val_loss: 0.7947\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1544 - val_loss: 0.7768\n",
      "Epoch 11/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1506 - val_loss: 0.7587\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.1486 - val_loss: 0.7405\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 0.1444 - val_loss: 0.7222\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.1391 - val_loss: 0.7041\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.1365 - val_loss: 0.6859\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.1306 - val_loss: 0.6677\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.1272 - val_loss: 0.6497\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.1220 - val_loss: 0.6318\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.1172 - val_loss: 0.6138\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1157 - val_loss: 0.5958\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.1124 - val_loss: 0.5780\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.1072 - val_loss: 0.5604\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 2s 54ms/step - loss: 0.1036 - val_loss: 0.5429\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0984 - val_loss: 0.5255\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 0.0954 - val_loss: 0.5082\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0908 - val_loss: 0.4911\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0875 - val_loss: 0.4741\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 0.0835 - val_loss: 0.4574\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0787 - val_loss: 0.4410\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0775 - val_loss: 0.4247\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 0.0740 - val_loss: 0.4087\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0690 - val_loss: 0.3930\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0663 - val_loss: 0.3774\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0626 - val_loss: 0.3622\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0597 - val_loss: 0.3473\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0580 - val_loss: 0.3325\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 2s 54ms/step - loss: 0.0545 - val_loss: 0.3181\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0510 - val_loss: 0.3042\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0486 - val_loss: 0.2905\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0449 - val_loss: 0.2773\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0422 - val_loss: 0.2644\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0398 - val_loss: 0.2519\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0380 - val_loss: 0.2396\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0343 - val_loss: 0.2280\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 2s 48ms/step - loss: 0.0325 - val_loss: 0.2166\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0312 - val_loss: 0.2055\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0283 - val_loss: 0.1951\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0266 - val_loss: 0.1850\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0249 - val_loss: 0.1754\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0229 - val_loss: 0.1661\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 0.0013\n",
      "Epoch 1/50\n",
      "31/31 [==============================] - 7s 84ms/step - loss: 0.0112 - val_loss: 1.5276\n",
      "Epoch 2/50\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.0109 - val_loss: 1.5143\n",
      "Epoch 3/50\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0106 - val_loss: 1.5010\n",
      "Epoch 4/50\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0103 - val_loss: 1.4876\n",
      "Epoch 5/50\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0101 - val_loss: 1.4742\n",
      "Epoch 6/50\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.0098 - val_loss: 1.4610\n",
      "Epoch 7/50\n",
      "31/31 [==============================] - 2s 65ms/step - loss: 0.0096 - val_loss: 1.4478\n",
      "Epoch 8/50\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0092 - val_loss: 1.4347\n",
      "Epoch 9/50\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0088 - val_loss: 1.4218\n",
      "Epoch 10/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0087 - val_loss: 1.4089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0084 - val_loss: 1.3961\n",
      "Epoch 12/50\n",
      "31/31 [==============================] - 1s 49ms/step - loss: 0.0082 - val_loss: 1.3834\n",
      "Epoch 13/50\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0080 - val_loss: 1.3709\n",
      "Epoch 14/50\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0077 - val_loss: 1.3583\n",
      "Epoch 15/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0075 - val_loss: 1.3458\n",
      "Epoch 16/50\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 0.0072 - val_loss: 1.3336\n",
      "Epoch 17/50\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0071 - val_loss: 1.3214\n",
      "Epoch 18/50\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.0069 - val_loss: 1.3094\n",
      "Epoch 19/50\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0066 - val_loss: 1.2975\n",
      "Epoch 20/50\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0065 - val_loss: 1.2857\n",
      "Epoch 21/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0064 - val_loss: 1.2739\n",
      "Epoch 22/50\n",
      "31/31 [==============================] - 2s 48ms/step - loss: 0.0060 - val_loss: 1.2625\n",
      "Epoch 23/50\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0058 - val_loss: 1.2513\n",
      "Epoch 24/50\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0056 - val_loss: 1.2401\n",
      "Epoch 25/50\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.0056 - val_loss: 1.2291\n",
      "Epoch 26/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0055 - val_loss: 1.2182\n",
      "Epoch 27/50\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 0.0052 - val_loss: 1.2076\n",
      "Epoch 28/50\n",
      "31/31 [==============================] - 2s 47ms/step - loss: 0.0051 - val_loss: 1.1971\n",
      "Epoch 29/50\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 0.0049 - val_loss: 1.1869\n",
      "Epoch 30/50\n",
      "31/31 [==============================] - 1s 49ms/step - loss: 0.0049 - val_loss: 1.1766\n",
      "Epoch 31/50\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0045 - val_loss: 1.1666\n",
      "Epoch 32/50\n",
      "31/31 [==============================] - 2s 51ms/step - loss: 0.0045 - val_loss: 1.1568\n",
      "Epoch 33/50\n",
      "31/31 [==============================] - 2s 53ms/step - loss: 0.0044 - val_loss: 1.1471\n",
      "Epoch 34/50\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0043 - val_loss: 1.1375\n",
      "Epoch 35/50\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0041 - val_loss: 1.1282\n",
      "Epoch 36/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0039 - val_loss: 1.1192\n",
      "Epoch 37/50\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0039 - val_loss: 1.1101\n",
      "Epoch 38/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0038 - val_loss: 1.1012\n",
      "Epoch 39/50\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 0.0036 - val_loss: 1.0925\n",
      "Epoch 40/50\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0035 - val_loss: 1.0840\n",
      "Epoch 41/50\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 0.0035 - val_loss: 1.0757\n",
      "Epoch 42/50\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0034 - val_loss: 1.0676\n",
      "Epoch 43/50\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 0.0033 - val_loss: 1.0596\n",
      "Epoch 44/50\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0032 - val_loss: 1.0519\n",
      "Epoch 45/50\n",
      "31/31 [==============================] - 2s 55ms/step - loss: 0.0032 - val_loss: 1.0443\n",
      "Epoch 46/50\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0031 - val_loss: 1.0370\n",
      "Epoch 47/50\n",
      "31/31 [==============================] - 2s 53ms/step - loss: 0.0031 - val_loss: 1.0298\n",
      "Epoch 48/50\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0030 - val_loss: 1.0228\n",
      "Epoch 49/50\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0030 - val_loss: 1.0158\n",
      "Epoch 50/50\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0029 - val_loss: 1.0092\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.1728\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 7s 92ms/step - loss: 0.0327 - val_loss: 0.0062\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0042 - val_loss: 0.0129\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0035 - val_loss: 0.0131\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0029 - val_loss: 0.0081\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0024 - val_loss: 0.0111\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0032 - val_loss: 0.0116\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0025 - val_loss: 0.0045\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0025 - val_loss: 0.0043\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0021 - val_loss: 0.0112\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0023 - val_loss: 0.0037\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0023 - val_loss: 0.0042\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0021 - val_loss: 0.0041\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0017 - val_loss: 0.0032\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0026 - val_loss: 0.0050\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0020 - val_loss: 0.0044\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0018 - val_loss: 0.0095\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0019 - val_loss: 0.0028\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0017 - val_loss: 0.0067\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 0.0019 - val_loss: 0.0055\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0018 - val_loss: 0.0033\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0019 - val_loss: 0.0040\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 0.0017 - val_loss: 0.0025\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0019 - val_loss: 0.0030\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 26/100\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 27/100\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0016 - val_loss: 0.0017\n",
      "Epoch 28/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0017 - val_loss: 0.0017\n",
      "Epoch 29/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0015 - val_loss: 0.0045\n",
      "Epoch 30/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0015 - val_loss: 0.0052\n",
      "Epoch 31/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0017 - val_loss: 0.0038\n",
      "Epoch 32/100\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0018 - val_loss: 0.0155\n",
      "Epoch 33/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0017 - val_loss: 0.0014\n",
      "Epoch 34/100\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 35/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0016 - val_loss: 0.0029\n",
      "Epoch 36/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0016 - val_loss: 0.0051\n",
      "Epoch 37/100\n",
      "31/31 [==============================] - 2s 54ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 38/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0016 - val_loss: 0.0092\n",
      "Epoch 39/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0016 - val_loss: 0.0063\n",
      "Epoch 40/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 41/100\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 0.0015 - val_loss: 0.0058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0014 - val_loss: 0.0013\n",
      "Epoch 43/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0013 - val_loss: 0.0017\n",
      "Epoch 44/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0012 - val_loss: 0.0037\n",
      "Epoch 45/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0012 - val_loss: 0.0053\n",
      "Epoch 46/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0012 - val_loss: 0.0016\n",
      "Epoch 47/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 48/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 49/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 0.0013 - val_loss: 0.0035\n",
      "Epoch 50/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0013 - val_loss: 0.0033\n",
      "Epoch 51/100\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 52/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0012 - val_loss: 0.0035\n",
      "Epoch 53/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0011 - val_loss: 0.0027\n",
      "Epoch 54/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 0.0012 - val_loss: 0.0027\n",
      "Epoch 55/100\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 56/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0012 - val_loss: 0.0065\n",
      "Epoch 57/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0013 - val_loss: 0.0012\n",
      "Epoch 58/100\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.0013 - val_loss: 0.0073\n",
      "Epoch 59/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.0012 - val_loss: 0.0034\n",
      "Epoch 60/100\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 0.0012 - val_loss: 0.0012\n",
      "Epoch 61/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.0013 - val_loss: 0.0034\n",
      "Epoch 62/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 63/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0011 - val_loss: 0.0023\n",
      "Epoch 64/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0011 - val_loss: 0.0022\n",
      "Epoch 65/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0010 - val_loss: 0.0028\n",
      "Epoch 66/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0012 - val_loss: 0.0068\n",
      "Epoch 67/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 0.0012 - val_loss: 8.5233e-04\n",
      "Epoch 68/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0010 - val_loss: 0.0038\n",
      "Epoch 69/100\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0011 - val_loss: 0.0093\n",
      "Epoch 70/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0013 - val_loss: 8.2431e-04\n",
      "Epoch 71/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 9.5803e-04 - val_loss: 0.0025\n",
      "Epoch 72/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0011 - val_loss: 0.0013\n",
      "Epoch 73/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0010 - val_loss: 0.0046\n",
      "Epoch 74/100\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 9.3972e-04 - val_loss: 0.0033\n",
      "Epoch 75/100\n",
      "31/31 [==============================] - 2s 54ms/step - loss: 9.4115e-04 - val_loss: 0.0032\n",
      "Epoch 76/100\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 8.9362e-04 - val_loss: 9.4637e-04\n",
      "Epoch 77/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0010 - val_loss: 0.0012\n",
      "Epoch 78/100\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 9.1680e-04 - val_loss: 0.0060\n",
      "Epoch 79/100\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 0.0011 - val_loss: 8.2428e-04\n",
      "Epoch 80/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 8.2456e-04 - val_loss: 0.0045\n",
      "Epoch 81/100\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 8.9691e-04 - val_loss: 0.0021\n",
      "Epoch 82/100\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 8.5105e-04 - val_loss: 0.0028\n",
      "Epoch 83/100\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 8.0967e-04 - val_loss: 0.0012\n",
      "Epoch 84/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 9.2171e-04 - val_loss: 0.0025\n",
      "Epoch 85/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 8.5769e-04 - val_loss: 0.0012\n",
      "Epoch 86/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 8.6202e-04 - val_loss: 0.0024\n",
      "Epoch 87/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 9.2534e-04 - val_loss: 0.0011\n",
      "Epoch 88/100\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 9.0955e-04 - val_loss: 0.0015\n",
      "Epoch 89/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 9.4620e-04 - val_loss: 9.8120e-04\n",
      "Epoch 90/100\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 9.4328e-04 - val_loss: 0.0013\n",
      "Epoch 91/100\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 9.3548e-04 - val_loss: 0.0023\n",
      "Epoch 92/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 8.0500e-04 - val_loss: 0.0021\n",
      "Epoch 93/100\n",
      "31/31 [==============================] - 2s 53ms/step - loss: 9.0840e-04 - val_loss: 0.0032\n",
      "Epoch 94/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 7.5467e-04 - val_loss: 0.0022\n",
      "Epoch 95/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 7.1991e-04 - val_loss: 0.0025\n",
      "Epoch 96/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 7.4648e-04 - val_loss: 7.4489e-04\n",
      "Epoch 97/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 7.6553e-04 - val_loss: 0.0038\n",
      "Epoch 98/100\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 8.9310e-04 - val_loss: 7.7354e-04\n",
      "Epoch 99/100\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 6.9462e-04 - val_loss: 8.5747e-04\n",
      "Epoch 100/100\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 7.4170e-04 - val_loss: 0.0030\n",
      "31/31 [==============================] - 1s 22ms/step - loss: 4.4998e-05\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 6s 71ms/step - loss: 0.0012 - val_loss: 0.1952\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 1.9867e-04 - val_loss: 0.1289\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.5479e-04 - val_loss: 0.1012\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 1.1918e-04 - val_loss: 0.0770\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 1.1394e-04 - val_loss: 0.0509\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 1.1054e-04 - val_loss: 0.0491\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.0986e-04 - val_loss: 0.0575\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 1.0038e-04 - val_loss: 0.0443\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 1.0032e-04 - val_loss: 0.0283\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 9.1174e-05 - val_loss: 0.0246\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 1s 49ms/step - loss: 9.4710e-05 - val_loss: 0.0281\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 8.4745e-05 - val_loss: 0.0336\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 7.8168e-05 - val_loss: 0.0244\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 7.1388e-05 - val_loss: 0.0217\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 7.7854e-05 - val_loss: 0.0198\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 7.6064e-05 - val_loss: 0.0276\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 6.4630e-05 - val_loss: 0.0174\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 7.3360e-05 - val_loss: 0.0224\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 1s 24ms/step - loss: 6.6291e-05 - val_loss: 0.0213\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 6.5848e-05 - val_loss: 0.0190\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 6.5551e-05 - val_loss: 0.0113\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 6.1901e-05 - val_loss: 0.0114\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 5.9584e-05 - val_loss: 0.0172\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 6.1964e-05 - val_loss: 0.0116\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 5.6497e-05 - val_loss: 0.0082\n",
      "Epoch 26/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 5.4223e-05 - val_loss: 0.0126\n",
      "Epoch 27/100\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 5.5030e-05 - val_loss: 0.0076\n",
      "Epoch 28/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 4.9900e-05 - val_loss: 0.0086\n",
      "Epoch 29/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 5.4156e-05 - val_loss: 0.0094\n",
      "Epoch 30/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 4.6237e-05 - val_loss: 0.0070\n",
      "Epoch 31/100\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 4.9738e-05 - val_loss: 0.0089\n",
      "Epoch 32/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 5.0736e-05 - val_loss: 0.0017\n",
      "Epoch 33/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 5.2073e-05 - val_loss: 0.0046\n",
      "Epoch 34/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 4.2083e-05 - val_loss: 0.0052\n",
      "Epoch 35/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 4.3625e-05 - val_loss: 0.0047\n",
      "Epoch 36/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 3.8691e-05 - val_loss: 0.0034\n",
      "Epoch 37/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 3.6116e-05 - val_loss: 0.0033\n",
      "Epoch 38/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 4.3081e-05 - val_loss: 0.0053\n",
      "Epoch 39/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 4.0655e-05 - val_loss: 0.0032\n",
      "Epoch 40/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 3.4902e-05 - val_loss: 0.0081\n",
      "Epoch 41/100\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 3.5365e-05 - val_loss: 0.0019\n",
      "Epoch 42/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 3.8903e-05 - val_loss: 0.0015\n",
      "Epoch 43/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.5301e-05 - val_loss: 0.0039\n",
      "Epoch 44/100\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 3.9738e-05 - val_loss: 0.0014\n",
      "Epoch 45/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 3.7526e-05 - val_loss: 0.0035\n",
      "Epoch 46/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 3.5164e-05 - val_loss: 0.0014\n",
      "Epoch 47/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 3.6862e-05 - val_loss: 0.0022\n",
      "Epoch 48/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.5012e-05 - val_loss: 0.0016\n",
      "Epoch 49/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.5870e-05 - val_loss: 0.0015\n",
      "Epoch 50/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 3.3693e-05 - val_loss: 0.0011\n",
      "Epoch 51/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 3.4057e-05 - val_loss: 0.0023\n",
      "Epoch 52/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 3.4444e-05 - val_loss: 0.0029\n",
      "Epoch 53/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 3.7940e-05 - val_loss: 8.1749e-04\n",
      "Epoch 54/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 3.7224e-05 - val_loss: 0.0021\n",
      "Epoch 55/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.0026e-05 - val_loss: 0.0019\n",
      "Epoch 56/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 3.1380e-05 - val_loss: 0.0013\n",
      "Epoch 57/100\n",
      "31/31 [==============================] - 2s 48ms/step - loss: 3.3382e-05 - val_loss: 0.0021\n",
      "Epoch 58/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 2.8672e-05 - val_loss: 0.0015\n",
      "Epoch 59/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 3.3085e-05 - val_loss: 7.1655e-04\n",
      "Epoch 60/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 3.3170e-05 - val_loss: 8.0856e-04\n",
      "Epoch 61/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 3.0096e-05 - val_loss: 0.0023\n",
      "Epoch 62/100\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 2.8095e-05 - val_loss: 0.0014\n",
      "Epoch 63/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.0589e-05 - val_loss: 0.0027\n",
      "Epoch 64/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 2.8764e-05 - val_loss: 0.0010\n",
      "Epoch 65/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 3.0878e-05 - val_loss: 0.0019\n",
      "Epoch 66/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 3.4898e-05 - val_loss: 6.5597e-04\n",
      "Epoch 67/100\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 2.8023e-05 - val_loss: 7.0046e-04\n",
      "Epoch 68/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 3.0473e-05 - val_loss: 0.0026\n",
      "Epoch 69/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 3.0214e-05 - val_loss: 0.0010\n",
      "Epoch 70/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 2.7480e-05 - val_loss: 0.0015\n",
      "Epoch 71/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 2.7395e-05 - val_loss: 0.0027\n",
      "Epoch 72/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 2.7013e-05 - val_loss: 0.0013\n",
      "Epoch 73/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 2.6582e-05 - val_loss: 0.0022\n",
      "Epoch 74/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 2.9168e-05 - val_loss: 0.0011\n",
      "Epoch 75/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 2.9027e-05 - val_loss: 0.0015\n",
      "Epoch 76/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 2.5402e-05 - val_loss: 9.0131e-04\n",
      "Epoch 77/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 2.5157e-05 - val_loss: 0.0012\n",
      "Epoch 78/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 2.7413e-05 - val_loss: 8.0213e-04\n",
      "Epoch 79/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 3.3443e-05 - val_loss: 0.0015\n",
      "Epoch 80/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 2.6402e-05 - val_loss: 0.0025\n",
      "Epoch 81/100\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 2.6122e-05 - val_loss: 8.9879e-04\n",
      "Epoch 82/100\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 2.4847e-05 - val_loss: 9.9598e-04\n",
      "Epoch 83/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 2.7741e-05 - val_loss: 8.5618e-04\n",
      "Epoch 84/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 2.8038e-05 - val_loss: 9.7033e-04\n",
      "Epoch 85/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 2.6114e-05 - val_loss: 0.0015\n",
      "Epoch 86/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 2.5165e-05 - val_loss: 6.4923e-04\n",
      "Epoch 87/100\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 2.4716e-05 - val_loss: 5.8104e-04\n",
      "Epoch 88/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 2.7222e-05 - val_loss: 9.8468e-04\n",
      "Epoch 89/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 3.0737e-05 - val_loss: 0.0037\n",
      "Epoch 90/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 2.6825e-05 - val_loss: 0.0011\n",
      "Epoch 91/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 2.8841e-05 - val_loss: 0.0028\n",
      "Epoch 92/100\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 2.6655e-05 - val_loss: 0.0013\n",
      "Epoch 93/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 2.3389e-05 - val_loss: 6.2498e-04\n",
      "Epoch 94/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 2.5398e-05 - val_loss: 4.8156e-04\n",
      "Epoch 95/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 2.3588e-05 - val_loss: 4.6941e-04\n",
      "Epoch 96/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 2.6001e-05 - val_loss: 6.1781e-04\n",
      "Epoch 97/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 2s 52ms/step - loss: 2.3720e-05 - val_loss: 8.5421e-04\n",
      "Epoch 98/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 2.4678e-05 - val_loss: 4.6348e-04\n",
      "Epoch 99/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 2.4003e-05 - val_loss: 4.5771e-04\n",
      "Epoch 100/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 2.5593e-05 - val_loss: 4.4659e-04\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 1.9288e-04\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 7s 94ms/step - loss: 0.1923 - val_loss: 0.9686\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 0.1857 - val_loss: 0.9491\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 0.1838 - val_loss: 0.9294\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.1773 - val_loss: 0.9099\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.1690 - val_loss: 0.8904\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.1691 - val_loss: 0.8711\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.1622 - val_loss: 0.8518\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 2s 55ms/step - loss: 0.1573 - val_loss: 0.8325\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 2s 54ms/step - loss: 0.1541 - val_loss: 0.8135\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 2s 47ms/step - loss: 0.1495 - val_loss: 0.7945\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.1473 - val_loss: 0.7755\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.1411 - val_loss: 0.7568\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.1378 - val_loss: 0.7381\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.1341 - val_loss: 0.7197\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.1273 - val_loss: 0.7014\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 2s 55ms/step - loss: 0.1242 - val_loss: 0.6832\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.1199 - val_loss: 0.6652\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.1175 - val_loss: 0.6474\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 0.1098 - val_loss: 0.6299\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.1076 - val_loss: 0.6125\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.1057 - val_loss: 0.5952\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 0.0997 - val_loss: 0.5781\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - 2s 46ms/step - loss: 0.0950 - val_loss: 0.5613\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0930 - val_loss: 0.5449\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 0.0912 - val_loss: 0.5285\n",
      "Epoch 26/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.0877 - val_loss: 0.5123\n",
      "Epoch 27/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 0.0819 - val_loss: 0.4965\n",
      "Epoch 28/100\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 0.0780 - val_loss: 0.4812\n",
      "Epoch 29/100\n",
      "31/31 [==============================] - 2s 54ms/step - loss: 0.0746 - val_loss: 0.4661\n",
      "Epoch 30/100\n",
      "31/31 [==============================] - 2s 55ms/step - loss: 0.0718 - val_loss: 0.4512\n",
      "Epoch 31/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0656 - val_loss: 0.4369\n",
      "Epoch 32/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0669 - val_loss: 0.4226\n",
      "Epoch 33/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0650 - val_loss: 0.4086\n",
      "Epoch 34/100\n",
      "31/31 [==============================] - 2s 51ms/step - loss: 0.0613 - val_loss: 0.3950\n",
      "Epoch 35/100\n",
      "31/31 [==============================] - 2s 51ms/step - loss: 0.0592 - val_loss: 0.3816\n",
      "Epoch 36/100\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0560 - val_loss: 0.3686\n",
      "Epoch 37/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0530 - val_loss: 0.3559\n",
      "Epoch 38/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0494 - val_loss: 0.3436\n",
      "Epoch 39/100\n",
      "31/31 [==============================] - 2s 65ms/step - loss: 0.0475 - val_loss: 0.3316\n",
      "Epoch 40/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0449 - val_loss: 0.3199\n",
      "Epoch 41/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0412 - val_loss: 0.3086\n",
      "Epoch 42/100\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0403 - val_loss: 0.2975\n",
      "Epoch 43/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0372 - val_loss: 0.2869\n",
      "Epoch 44/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0368 - val_loss: 0.2764\n",
      "Epoch 45/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0339 - val_loss: 0.2663\n",
      "Epoch 46/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.0343 - val_loss: 0.2563\n",
      "Epoch 47/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0303 - val_loss: 0.2471\n",
      "Epoch 48/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0311 - val_loss: 0.2379\n",
      "Epoch 49/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0295 - val_loss: 0.2291\n",
      "Epoch 50/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0279 - val_loss: 0.2205\n",
      "Epoch 51/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0260 - val_loss: 0.2123\n",
      "Epoch 52/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0249 - val_loss: 0.2042\n",
      "Epoch 53/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0245 - val_loss: 0.1964\n",
      "Epoch 54/100\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0236 - val_loss: 0.1891\n",
      "Epoch 55/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0213 - val_loss: 0.1822\n",
      "Epoch 56/100\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.0199 - val_loss: 0.1756\n",
      "Epoch 57/100\n",
      "31/31 [==============================] - 2s 58ms/step - loss: 0.0195 - val_loss: 0.1693\n",
      "Epoch 58/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0179 - val_loss: 0.1634\n",
      "Epoch 59/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0164 - val_loss: 0.1578\n",
      "Epoch 60/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0185 - val_loss: 0.1519\n",
      "Epoch 61/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0163 - val_loss: 0.1467\n",
      "Epoch 62/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.0159 - val_loss: 0.1419\n",
      "Epoch 63/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0170 - val_loss: 0.1369\n",
      "Epoch 64/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0155 - val_loss: 0.1324\n",
      "Epoch 65/100\n",
      "31/31 [==============================] - 1s 49ms/step - loss: 0.0143 - val_loss: 0.1283\n",
      "Epoch 66/100\n",
      "31/31 [==============================] - 2s 74ms/step - loss: 0.0126 - val_loss: 0.1247\n",
      "Epoch 67/100\n",
      "31/31 [==============================] - 2s 54ms/step - loss: 0.0132 - val_loss: 0.1211\n",
      "Epoch 68/100\n",
      "31/31 [==============================] - 2s 47ms/step - loss: 0.0128 - val_loss: 0.1178\n",
      "Epoch 69/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.0131 - val_loss: 0.1144\n",
      "Epoch 70/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.0138 - val_loss: 0.1113\n",
      "Epoch 71/100\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0124 - val_loss: 0.1086\n",
      "Epoch 72/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0129 - val_loss: 0.1056\n",
      "Epoch 73/100\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0109 - val_loss: 0.1032\n",
      "Epoch 74/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0129 - val_loss: 0.1007\n",
      "Epoch 75/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0125 - val_loss: 0.0983\n",
      "Epoch 76/100\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0138 - val_loss: 0.0958\n",
      "Epoch 77/100\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0124 - val_loss: 0.0935\n",
      "Epoch 78/100\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 0.0119 - val_loss: 0.0916\n",
      "Epoch 79/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0119 - val_loss: 0.0901\n",
      "Epoch 80/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0118 - val_loss: 0.0883\n",
      "Epoch 81/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.0110 - val_loss: 0.0868\n",
      "Epoch 82/100\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 0.0113 - val_loss: 0.0856\n",
      "Epoch 83/100\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0104 - val_loss: 0.0844\n",
      "Epoch 84/100\n",
      "31/31 [==============================] - 2s 48ms/step - loss: 0.0119 - val_loss: 0.0828\n",
      "Epoch 85/100\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 0.0109 - val_loss: 0.0814\n",
      "Epoch 86/100\n",
      "31/31 [==============================] - 2s 63ms/step - loss: 0.0114 - val_loss: 0.0800\n",
      "Epoch 87/100\n",
      "31/31 [==============================] - 2s 72ms/step - loss: 0.0108 - val_loss: 0.0786\n",
      "Epoch 88/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0108 - val_loss: 0.0777\n",
      "Epoch 89/100\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 0.0107 - val_loss: 0.0768\n",
      "Epoch 90/100\n",
      "31/31 [==============================] - 2s 53ms/step - loss: 0.0101 - val_loss: 0.0756\n",
      "Epoch 91/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 0.0107 - val_loss: 0.0751\n",
      "Epoch 92/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0107 - val_loss: 0.0744\n",
      "Epoch 93/100\n",
      "31/31 [==============================] - 2s 66ms/step - loss: 0.0107 - val_loss: 0.0735\n",
      "Epoch 94/100\n",
      "31/31 [==============================] - 2s 65ms/step - loss: 0.0103 - val_loss: 0.0725\n",
      "Epoch 95/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0109 - val_loss: 0.0715\n",
      "Epoch 96/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0097 - val_loss: 0.0708\n",
      "Epoch 97/100\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.0097 - val_loss: 0.0698\n",
      "Epoch 98/100\n",
      "31/31 [==============================] - 2s 67ms/step - loss: 0.0103 - val_loss: 0.0692\n",
      "Epoch 99/100\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.0105 - val_loss: 0.0682\n",
      "Epoch 100/100\n",
      "31/31 [==============================] - 2s 66ms/step - loss: 0.0090 - val_loss: 0.0673\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0012\n",
      "Epoch 1/100\n",
      "31/31 [==============================] - 7s 89ms/step - loss: 0.0130 - val_loss: 1.4518\n",
      "Epoch 2/100\n",
      "31/31 [==============================] - 2s 51ms/step - loss: 0.0127 - val_loss: 1.4410\n",
      "Epoch 3/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0125 - val_loss: 1.4302\n",
      "Epoch 4/100\n",
      "31/31 [==============================] - 2s 51ms/step - loss: 0.0122 - val_loss: 1.4192\n",
      "Epoch 5/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0117 - val_loss: 1.4083\n",
      "Epoch 6/100\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 0.0116 - val_loss: 1.3973\n",
      "Epoch 7/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0113 - val_loss: 1.3863\n",
      "Epoch 8/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0111 - val_loss: 1.3753\n",
      "Epoch 9/100\n",
      "31/31 [==============================] - 2s 57ms/step - loss: 0.0109 - val_loss: 1.3642\n",
      "Epoch 10/100\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 0.0105 - val_loss: 1.3533\n",
      "Epoch 11/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0102 - val_loss: 1.3423\n",
      "Epoch 12/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0099 - val_loss: 1.3315\n",
      "Epoch 13/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0097 - val_loss: 1.3207\n",
      "Epoch 14/100\n",
      "31/31 [==============================] - 2s 55ms/step - loss: 0.0096 - val_loss: 1.3098\n",
      "Epoch 15/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0093 - val_loss: 1.2990\n",
      "Epoch 16/100\n",
      "31/31 [==============================] - 1s 39ms/step - loss: 0.0091 - val_loss: 1.2884\n",
      "Epoch 17/100\n",
      "31/31 [==============================] - 1s 36ms/step - loss: 0.0087 - val_loss: 1.2778\n",
      "Epoch 18/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0085 - val_loss: 1.2673\n",
      "Epoch 19/100\n",
      "31/31 [==============================] - 1s 29ms/step - loss: 0.0084 - val_loss: 1.2569\n",
      "Epoch 20/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0081 - val_loss: 1.2466\n",
      "Epoch 21/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0078 - val_loss: 1.2364\n",
      "Epoch 22/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0077 - val_loss: 1.2263\n",
      "Epoch 23/100\n",
      "31/31 [==============================] - 1s 44ms/step - loss: 0.0076 - val_loss: 1.2161\n",
      "Epoch 24/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0072 - val_loss: 1.2063\n",
      "Epoch 25/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.0071 - val_loss: 1.1964\n",
      "Epoch 26/100\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 0.0069 - val_loss: 1.1867\n",
      "Epoch 27/100\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0067 - val_loss: 1.1770\n",
      "Epoch 28/100\n",
      "31/31 [==============================] - 1s 25ms/step - loss: 0.0066 - val_loss: 1.1675\n",
      "Epoch 29/100\n",
      "31/31 [==============================] - 2s 53ms/step - loss: 0.0065 - val_loss: 1.1580\n",
      "Epoch 30/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0062 - val_loss: 1.1488\n",
      "Epoch 31/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0060 - val_loss: 1.1396\n",
      "Epoch 32/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0059 - val_loss: 1.1306\n",
      "Epoch 33/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0057 - val_loss: 1.1216\n",
      "Epoch 34/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0055 - val_loss: 1.1128\n",
      "Epoch 35/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0054 - val_loss: 1.1041\n",
      "Epoch 36/100\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0052 - val_loss: 1.0955\n",
      "Epoch 37/100\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0051 - val_loss: 1.0869\n",
      "Epoch 38/100\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0050 - val_loss: 1.0785\n",
      "Epoch 39/100\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0049 - val_loss: 1.0702\n",
      "Epoch 40/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 0.0046 - val_loss: 1.0622\n",
      "Epoch 41/100\n",
      "31/31 [==============================] - 1s 31ms/step - loss: 0.0046 - val_loss: 1.0542\n",
      "Epoch 42/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 0.0044 - val_loss: 1.0464\n",
      "Epoch 43/100\n",
      "31/31 [==============================] - 2s 64ms/step - loss: 0.0043 - val_loss: 1.0387\n",
      "Epoch 44/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 0.0043 - val_loss: 1.0310\n",
      "Epoch 45/100\n",
      "31/31 [==============================] - 2s 61ms/step - loss: 0.0040 - val_loss: 1.0235\n",
      "Epoch 46/100\n",
      "31/31 [==============================] - 2s 49ms/step - loss: 0.0041 - val_loss: 1.0160\n",
      "Epoch 47/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0039 - val_loss: 1.0088\n",
      "Epoch 48/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0038 - val_loss: 1.0015\n",
      "Epoch 49/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 0.0037 - val_loss: 0.9945\n",
      "Epoch 50/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0037 - val_loss: 0.9876\n",
      "Epoch 51/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0035 - val_loss: 0.9807\n",
      "Epoch 52/100\n",
      "31/31 [==============================] - 2s 51ms/step - loss: 0.0035 - val_loss: 0.9739\n",
      "Epoch 53/100\n",
      "31/31 [==============================] - 2s 59ms/step - loss: 0.0034 - val_loss: 0.9674\n",
      "Epoch 54/100\n",
      "31/31 [==============================] - 1s 32ms/step - loss: 0.0032 - val_loss: 0.9610\n",
      "Epoch 55/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0032 - val_loss: 0.9547\n",
      "Epoch 56/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 0.0032 - val_loss: 0.9484\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 44ms/step - loss: 0.0030 - val_loss: 0.9424\n",
      "Epoch 58/100\n",
      "31/31 [==============================] - 2s 53ms/step - loss: 0.0031 - val_loss: 0.9363\n",
      "Epoch 59/100\n",
      "31/31 [==============================] - 2s 53ms/step - loss: 0.0030 - val_loss: 0.9305\n",
      "Epoch 60/100\n",
      "31/31 [==============================] - 2s 48ms/step - loss: 0.0029 - val_loss: 0.9248\n",
      "Epoch 61/100\n",
      "31/31 [==============================] - 1s 46ms/step - loss: 0.0029 - val_loss: 0.9191\n",
      "Epoch 62/100\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 0.0027 - val_loss: 0.9137\n",
      "Epoch 63/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0028 - val_loss: 0.9083\n",
      "Epoch 64/100\n",
      "31/31 [==============================] - 2s 55ms/step - loss: 0.0027 - val_loss: 0.9030\n",
      "Epoch 65/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0027 - val_loss: 0.8977\n",
      "Epoch 66/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0026 - val_loss: 0.8926\n",
      "Epoch 67/100\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 0.0026 - val_loss: 0.8877\n",
      "Epoch 68/100\n",
      "31/31 [==============================] - 2s 60ms/step - loss: 0.0026 - val_loss: 0.8828\n",
      "Epoch 69/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0025 - val_loss: 0.8781\n",
      "Epoch 70/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0025 - val_loss: 0.8734\n",
      "Epoch 71/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0025 - val_loss: 0.8688\n",
      "Epoch 72/100\n",
      "31/31 [==============================] - 2s 56ms/step - loss: 0.0023 - val_loss: 0.8643\n",
      "Epoch 73/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0024 - val_loss: 0.8601\n",
      "Epoch 74/100\n",
      "31/31 [==============================] - 2s 53ms/step - loss: 0.0023 - val_loss: 0.8560\n",
      "Epoch 75/100\n",
      "31/31 [==============================] - 1s 42ms/step - loss: 0.0023 - val_loss: 0.8519\n",
      "Epoch 76/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0023 - val_loss: 0.8477\n",
      "Epoch 77/100\n",
      "31/31 [==============================] - 1s 38ms/step - loss: 0.0023 - val_loss: 0.8437\n",
      "Epoch 78/100\n",
      "31/31 [==============================] - 1s 33ms/step - loss: 0.0023 - val_loss: 0.8398\n",
      "Epoch 79/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.0023 - val_loss: 0.8360\n",
      "Epoch 80/100\n",
      "31/31 [==============================] - 1s 28ms/step - loss: 0.0021 - val_loss: 0.8324\n",
      "Epoch 81/100\n",
      "31/31 [==============================] - 1s 26ms/step - loss: 0.0022 - val_loss: 0.8288\n",
      "Epoch 82/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0021 - val_loss: 0.8254\n",
      "Epoch 83/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0021 - val_loss: 0.8220\n",
      "Epoch 84/100\n",
      "31/31 [==============================] - 1s 47ms/step - loss: 0.0022 - val_loss: 0.8187\n",
      "Epoch 85/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0022 - val_loss: 0.8152\n",
      "Epoch 86/100\n",
      "31/31 [==============================] - 1s 40ms/step - loss: 0.0021 - val_loss: 0.8120\n",
      "Epoch 87/100\n",
      "31/31 [==============================] - 2s 52ms/step - loss: 0.0022 - val_loss: 0.8089\n",
      "Epoch 88/100\n",
      "31/31 [==============================] - 1s 35ms/step - loss: 0.0021 - val_loss: 0.8057\n",
      "Epoch 89/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 0.0020 - val_loss: 0.8028\n",
      "Epoch 90/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0020 - val_loss: 0.7999\n",
      "Epoch 91/100\n",
      "31/31 [==============================] - 1s 41ms/step - loss: 0.0020 - val_loss: 0.7969\n",
      "Epoch 92/100\n",
      "31/31 [==============================] - 1s 43ms/step - loss: 0.0020 - val_loss: 0.7940\n",
      "Epoch 93/100\n",
      "31/31 [==============================] - 2s 50ms/step - loss: 0.0020 - val_loss: 0.7912\n",
      "Epoch 94/100\n",
      "31/31 [==============================] - 1s 49ms/step - loss: 0.0020 - val_loss: 0.7884\n",
      "Epoch 95/100\n",
      "31/31 [==============================] - 1s 45ms/step - loss: 0.0020 - val_loss: 0.7857\n",
      "Epoch 96/100\n",
      "31/31 [==============================] - 1s 48ms/step - loss: 0.0019 - val_loss: 0.7830\n",
      "Epoch 97/100\n",
      "31/31 [==============================] - 2s 54ms/step - loss: 0.0020 - val_loss: 0.7804\n",
      "Epoch 98/100\n",
      "31/31 [==============================] - 1s 34ms/step - loss: 0.0020 - val_loss: 0.7778\n",
      "Epoch 99/100\n",
      "31/31 [==============================] - 1s 27ms/step - loss: 0.0020 - val_loss: 0.7753\n",
      "Epoch 100/100\n",
      "31/31 [==============================] - 1s 37ms/step - loss: 0.0021 - val_loss: 0.7727\n",
      "31/31 [==============================] - 1s 30ms/step - loss: 0.1332\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 8s 136ms/step - loss: 0.0534 - val_loss: 0.0948\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0087 - val_loss: 0.0081\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.0042 - val_loss: 0.0108\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0035 - val_loss: 0.0092\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0029 - val_loss: 0.0067\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0030 - val_loss: 0.0073\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0028 - val_loss: 0.0052\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.0025 - val_loss: 0.0039\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 2s 92ms/step - loss: 0.0022 - val_loss: 0.0046\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0025 - val_loss: 0.0048\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0024 - val_loss: 0.0031\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0024 - val_loss: 0.0050\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0021 - val_loss: 0.0034\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.0020 - val_loss: 0.0046\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0020 - val_loss: 0.0025\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0024 - val_loss: 0.0040\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0020 - val_loss: 0.0070\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0022 - val_loss: 0.0035\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0020 - val_loss: 0.0040\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0019 - val_loss: 0.0062\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.0019 - val_loss: 0.0026\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0017 - val_loss: 0.0040\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0020 - val_loss: 0.0071\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.0015 - val_loss: 0.0023\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.0018 - val_loss: 0.0019\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0018 - val_loss: 0.0029\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0020 - val_loss: 0.0024\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0018 - val_loss: 0.0042\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0021 - val_loss: 0.0037\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 2s 92ms/step - loss: 0.0018 - val_loss: 0.0024\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0015 - val_loss: 0.0037\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0016 - val_loss: 0.0024\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.0014 - val_loss: 0.0024\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0018 - val_loss: 0.0024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.0015 - val_loss: 0.0036\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.0017 - val_loss: 0.0043\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.0019 - val_loss: 0.0015\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 0.0015 - val_loss: 0.0032\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 0.0015 - val_loss: 0.0015\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0018 - val_loss: 0.0052\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 1s 95ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0018 - val_loss: 0.0050\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0018 - val_loss: 0.0034\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.0015 - val_loss: 0.0020\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0016 - val_loss: 0.0015\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0017 - val_loss: 0.0023\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 9.5309e-05\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 7s 152ms/step - loss: 0.0017 - val_loss: 0.1163\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 3.6124e-04 - val_loss: 0.1963\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 2.1785e-04 - val_loss: 0.1467\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 1.7424e-04 - val_loss: 0.1207\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 1.5955e-04 - val_loss: 0.1011\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 1.3611e-04 - val_loss: 0.0694\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 1.1943e-04 - val_loss: 0.0582\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 1.2299e-04 - val_loss: 0.0385\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 1.1156e-04 - val_loss: 0.0348\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 1.0882e-04 - val_loss: 0.0275\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 1.0635e-04 - val_loss: 0.0235\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 1.0333e-04 - val_loss: 0.0245\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 9.2497e-05 - val_loss: 0.0180\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 9.7730e-05 - val_loss: 0.0253\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 8.5300e-05 - val_loss: 0.0185\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 8.7728e-05 - val_loss: 0.0192\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 1.0401e-04 - val_loss: 0.0183\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 1.1128e-04 - val_loss: 0.0161\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 9.1377e-05 - val_loss: 0.0185\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 9.1397e-05 - val_loss: 0.0138\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 7.7468e-05 - val_loss: 0.0196\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 8.3394e-05 - val_loss: 0.0126\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 7.4429e-05 - val_loss: 0.0114\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 7.9492e-05 - val_loss: 0.0217\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 7.3673e-05 - val_loss: 0.0154\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 7.6861e-05 - val_loss: 0.0144\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 6.9705e-05 - val_loss: 0.0200\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 7.6715e-05 - val_loss: 0.0227\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 6.9220e-05 - val_loss: 0.0176\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 6.4097e-05 - val_loss: 0.0078\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 6.9906e-05 - val_loss: 0.0149\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 6.8479e-05 - val_loss: 0.0141\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 6.5528e-05 - val_loss: 0.0079\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 0s 23ms/step - loss: 6.5537e-05 - val_loss: 0.0092\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 6.0608e-05 - val_loss: 0.0097\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 6.4023e-05 - val_loss: 0.0092\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 6.7181e-05 - val_loss: 0.0029\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 6.5392e-05 - val_loss: 0.0057\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 6.2548e-05 - val_loss: 0.0104\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 5.8062e-05 - val_loss: 0.0052\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 5.3062e-05 - val_loss: 0.0037\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 5.7801e-05 - val_loss: 0.0045\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 0s 25ms/step - loss: 6.1114e-05 - val_loss: 0.0036\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 0s 24ms/step - loss: 5.1285e-05 - val_loss: 0.0044\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 5.3246e-05 - val_loss: 0.0047\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 1s 39ms/step - loss: 4.7562e-05 - val_loss: 0.0044\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 5.7320e-05 - val_loss: 0.0063\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 5.3648e-05 - val_loss: 0.0041\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 5.7805e-05 - val_loss: 0.0019\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 4.7229e-05 - val_loss: 0.0019\n",
      "16/16 [==============================] - 1s 46ms/step - loss: 3.2752e-04\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 7s 146ms/step - loss: 0.1954 - val_loss: 0.9225\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.1975 - val_loss: 0.9133\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.1908 - val_loss: 0.9040\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.1933 - val_loss: 0.8945\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.1896 - val_loss: 0.8851\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.1834 - val_loss: 0.8758\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.1838 - val_loss: 0.8662\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.1798 - val_loss: 0.8567\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.1794 - val_loss: 0.8469\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.1775 - val_loss: 0.8372\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.1769 - val_loss: 0.8274\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 0.1725 - val_loss: 0.8177\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.1726 - val_loss: 0.8079\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.1664 - val_loss: 0.7981\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.1643 - val_loss: 0.7883\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.1610 - val_loss: 0.7785\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.1575 - val_loss: 0.7687\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.1578 - val_loss: 0.7590\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.1604 - val_loss: 0.7491\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.1519 - val_loss: 0.7391\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.1507 - val_loss: 0.7292\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.1474 - val_loss: 0.7194\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.1458 - val_loss: 0.7096\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.1390 - val_loss: 0.6998\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.1402 - val_loss: 0.6901\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.1368 - val_loss: 0.6803\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.1317 - val_loss: 0.6706\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 0.1300 - val_loss: 0.6609\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.1317 - val_loss: 0.6511\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.1282 - val_loss: 0.6414\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.1240 - val_loss: 0.6317\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.1225 - val_loss: 0.6221\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.1224 - val_loss: 0.6125\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.1191 - val_loss: 0.6028\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.1157 - val_loss: 0.5933\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.1123 - val_loss: 0.5839\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.1094 - val_loss: 0.5745\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.1098 - val_loss: 0.5652\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 1s 96ms/step - loss: 0.1085 - val_loss: 0.5556\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.1027 - val_loss: 0.5463\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.1040 - val_loss: 0.5370\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 0.0996 - val_loss: 0.5277\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0990 - val_loss: 0.5185\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0975 - val_loss: 0.5093\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.0938 - val_loss: 0.5002\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.0911 - val_loss: 0.4911\n",
      "Epoch 47/50\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0895 - val_loss: 0.4820\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0873 - val_loss: 0.4731\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0844 - val_loss: 0.4641\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0841 - val_loss: 0.4554\n",
      "16/16 [==============================] - 1s 38ms/step - loss: 0.0019\n",
      "Epoch 1/50\n",
      "16/16 [==============================] - 7s 161ms/step - loss: 0.0176 - val_loss: 1.6153\n",
      "Epoch 2/50\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0174 - val_loss: 1.6080\n",
      "Epoch 3/50\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0173 - val_loss: 1.6006\n",
      "Epoch 4/50\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0170 - val_loss: 1.5933\n",
      "Epoch 5/50\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.0166 - val_loss: 1.5858\n",
      "Epoch 6/50\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0166 - val_loss: 1.5783\n",
      "Epoch 7/50\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0165 - val_loss: 1.5708\n",
      "Epoch 8/50\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0162 - val_loss: 1.5632\n",
      "Epoch 9/50\n",
      "16/16 [==============================] - 1s 50ms/step - loss: 0.0161 - val_loss: 1.5556\n",
      "Epoch 10/50\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0158 - val_loss: 1.5480\n",
      "Epoch 11/50\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0159 - val_loss: 1.5403\n",
      "Epoch 12/50\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0155 - val_loss: 1.5326\n",
      "Epoch 13/50\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0153 - val_loss: 1.5250\n",
      "Epoch 14/50\n",
      "16/16 [==============================] - 2s 92ms/step - loss: 0.0150 - val_loss: 1.5173\n",
      "Epoch 15/50\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.0150 - val_loss: 1.5096\n",
      "Epoch 16/50\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.0147 - val_loss: 1.5020\n",
      "Epoch 17/50\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0146 - val_loss: 1.4943\n",
      "Epoch 18/50\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0142 - val_loss: 1.4868\n",
      "Epoch 19/50\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.0141 - val_loss: 1.4792\n",
      "Epoch 20/50\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0140 - val_loss: 1.4716\n",
      "Epoch 21/50\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0137 - val_loss: 1.4640\n",
      "Epoch 22/50\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0135 - val_loss: 1.4565\n",
      "Epoch 23/50\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0134 - val_loss: 1.4490\n",
      "Epoch 24/50\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.0135 - val_loss: 1.4414\n",
      "Epoch 25/50\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0130 - val_loss: 1.4339\n",
      "Epoch 26/50\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0129 - val_loss: 1.4264\n",
      "Epoch 27/50\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0128 - val_loss: 1.4190\n",
      "Epoch 28/50\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0128 - val_loss: 1.4114\n",
      "Epoch 29/50\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0124 - val_loss: 1.4040\n",
      "Epoch 30/50\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0121 - val_loss: 1.3968\n",
      "Epoch 31/50\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0120 - val_loss: 1.3894\n",
      "Epoch 32/50\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 0.0118 - val_loss: 1.3821\n",
      "Epoch 33/50\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0116 - val_loss: 1.3749\n",
      "Epoch 34/50\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0114 - val_loss: 1.3676\n",
      "Epoch 35/50\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0113 - val_loss: 1.3605\n",
      "Epoch 36/50\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0113 - val_loss: 1.3533\n",
      "Epoch 37/50\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.0111 - val_loss: 1.3460\n",
      "Epoch 38/50\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0108 - val_loss: 1.3389\n",
      "Epoch 39/50\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0108 - val_loss: 1.3318\n",
      "Epoch 40/50\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0108 - val_loss: 1.3245\n",
      "Epoch 41/50\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0104 - val_loss: 1.3174\n",
      "Epoch 42/50\n",
      "16/16 [==============================] - 1s 41ms/step - loss: 0.0103 - val_loss: 1.3103\n",
      "Epoch 43/50\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0102 - val_loss: 1.3032\n",
      "Epoch 44/50\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0099 - val_loss: 1.2963\n",
      "Epoch 45/50\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0097 - val_loss: 1.2894\n",
      "Epoch 46/50\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0098 - val_loss: 1.2824\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 54ms/step - loss: 0.0096 - val_loss: 1.2756\n",
      "Epoch 48/50\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0094 - val_loss: 1.2688\n",
      "Epoch 49/50\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0092 - val_loss: 1.2620\n",
      "Epoch 50/50\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0092 - val_loss: 1.2552\n",
      "16/16 [==============================] - 0s 18ms/step - loss: 0.2619\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 8s 140ms/step - loss: 0.0301 - val_loss: 0.0917\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.0066 - val_loss: 0.0261\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0045 - val_loss: 0.0108\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0036 - val_loss: 0.0095\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0030 - val_loss: 0.0067\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 0.0030 - val_loss: 0.0034\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 0.0030 - val_loss: 0.0038\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0022 - val_loss: 0.0055\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0025 - val_loss: 0.0058\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.0022 - val_loss: 0.0042\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.0024 - val_loss: 0.0032\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.0023 - val_loss: 0.0069\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0023 - val_loss: 0.0030\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0019 - val_loss: 0.0067\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0021 - val_loss: 0.0036\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0021 - val_loss: 0.0044\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0022 - val_loss: 0.0021\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0021 - val_loss: 0.0060\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0020 - val_loss: 0.0075\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0020 - val_loss: 0.0039\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0019 - val_loss: 0.0029\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0020 - val_loss: 0.0058\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 2s 91ms/step - loss: 0.0020 - val_loss: 0.0071\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0019 - val_loss: 0.0035\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.0017 - val_loss: 0.0026\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0018 - val_loss: 0.0039\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0019 - val_loss: 0.0049\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0019 - val_loss: 0.0022\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.0018 - val_loss: 0.0017\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0017 - val_loss: 0.0072\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 2s 87ms/step - loss: 0.0018 - val_loss: 0.0023\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0018 - val_loss: 0.0066\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0017 - val_loss: 0.0056\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0017 - val_loss: 0.0029\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0017 - val_loss: 0.0016\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0016 - val_loss: 0.0018\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.0016 - val_loss: 0.0060\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0017 - val_loss: 0.0018\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0015 - val_loss: 0.0028\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0016 - val_loss: 0.0021\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.0014 - val_loss: 0.0037\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0015 - val_loss: 0.0018\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0015 - val_loss: 0.0050\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0015 - val_loss: 0.0014\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.0016 - val_loss: 0.0025\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0014 - val_loss: 0.0040\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0013 - val_loss: 0.0021\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0014 - val_loss: 0.0012\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 0.0014\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0014 - val_loss: 0.0045\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.0015 - val_loss: 0.0012\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0014 - val_loss: 0.0014\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0013 - val_loss: 0.0023\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.0013 - val_loss: 0.0020\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0014 - val_loss: 0.0011\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0014 - val_loss: 0.0037\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0014 - val_loss: 0.0038\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.0012 - val_loss: 0.0010\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0012 - val_loss: 0.0019\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0013 - val_loss: 0.0043\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0013 - val_loss: 0.0013\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.0011 - val_loss: 0.0042\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.0012 - val_loss: 0.0013\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0014 - val_loss: 0.0015\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.0015 - val_loss: 0.0024\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0013 - val_loss: 0.0015\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0012 - val_loss: 0.0011\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 1s 96ms/step - loss: 0.0012 - val_loss: 0.0051\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0011 - val_loss: 0.0029\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.0013 - val_loss: 0.0045\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.0011 - val_loss: 0.0016\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0014 - val_loss: 0.0017\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0013 - val_loss: 0.0029\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0011 - val_loss: 0.0012\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0011 - val_loss: 0.0031\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0011 - val_loss: 0.0011\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0012 - val_loss: 0.0041\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0012 - val_loss: 0.0014\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0012 - val_loss: 0.0021\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0013 - val_loss: 9.2983e-04\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 0.0010 - val_loss: 0.0017\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.0010 - val_loss: 0.0013\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0011 - val_loss: 0.0017\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0010 - val_loss: 0.0016\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.0011 - val_loss: 0.0048\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0010 - val_loss: 8.8402e-04\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0012 - val_loss: 0.0045\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.0012 - val_loss: 0.0031\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 2s 91ms/step - loss: 0.0011 - val_loss: 0.0014\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.0012 - val_loss: 9.7426e-04\n",
      "16/16 [==============================] - 1s 35ms/step - loss: 8.4667e-05\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 8s 155ms/step - loss: 0.0024 - val_loss: 0.2920\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 4.2023e-04 - val_loss: 0.1865\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 2.3051e-04 - val_loss: 0.1492\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 1.7734e-04 - val_loss: 0.1286\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 2s 92ms/step - loss: 1.5350e-04 - val_loss: 0.1136\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 1.2802e-04 - val_loss: 0.0947\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 1.2172e-04 - val_loss: 0.0771\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 2s 89ms/step - loss: 1.1602e-04 - val_loss: 0.0825\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 1.1812e-04 - val_loss: 0.0659\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 1.1367e-04 - val_loss: 0.0667\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 9.8996e-05 - val_loss: 0.0584\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 1s 62ms/step - loss: 1.0317e-04 - val_loss: 0.0512\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 9.4873e-05 - val_loss: 0.0471\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 9.3441e-05 - val_loss: 0.0393\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 9.0463e-05 - val_loss: 0.0398\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 9.8826e-05 - val_loss: 0.0323\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 8.8413e-05 - val_loss: 0.0305\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 8.6327e-05 - val_loss: 0.0383\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 8.8526e-05 - val_loss: 0.0276\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 8.6574e-05 - val_loss: 0.0321\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 8.7807e-05 - val_loss: 0.0245\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 8.2325e-05 - val_loss: 0.0302\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 7.7592e-05 - val_loss: 0.0285\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 7.7207e-05 - val_loss: 0.0267\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 7.2909e-05 - val_loss: 0.0229\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 7.7150e-05 - val_loss: 0.0220\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 7.5871e-05 - val_loss: 0.0182\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 8.4100e-05 - val_loss: 0.0145\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 7.4853e-05 - val_loss: 0.0182\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 1s 96ms/step - loss: 7.4177e-05 - val_loss: 0.0186\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 7.1910e-05 - val_loss: 0.0128\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 7.2976e-05 - val_loss: 0.0263\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 2s 95ms/step - loss: 7.2105e-05 - val_loss: 0.0118\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 7.2713e-05 - val_loss: 0.0225\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 6.8860e-05 - val_loss: 0.0138\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 7.1852e-05 - val_loss: 0.0159\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 6.6481e-05 - val_loss: 0.0127\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 6.5041e-05 - val_loss: 0.0135\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 6.2978e-05 - val_loss: 0.0123\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 6.1543e-05 - val_loss: 0.0115\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 5.9650e-05 - val_loss: 0.0093\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 5.5784e-05 - val_loss: 0.0124\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 5.8613e-05 - val_loss: 0.0126\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 6.1433e-05 - val_loss: 0.0076\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 5.6494e-05 - val_loss: 0.0044\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 5.0829e-05 - val_loss: 0.0065\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 5.5992e-05 - val_loss: 0.0067\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 5.8532e-05 - val_loss: 0.0047\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 5.4180e-05 - val_loss: 0.0044\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 4.6104e-05 - val_loss: 0.0022\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 1s 96ms/step - loss: 5.2412e-05 - val_loss: 0.0029\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 5.6683e-05 - val_loss: 0.0026\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 5.2429e-05 - val_loss: 0.0021\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 5.4444e-05 - val_loss: 0.0017\n",
      "Epoch 55/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 2s 107ms/step - loss: 4.7637e-05 - val_loss: 0.0015\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 4.8978e-05 - val_loss: 0.0011\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 4.6513e-05 - val_loss: 0.0020\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 4.7999e-05 - val_loss: 0.0012\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 4.2829e-05 - val_loss: 9.7060e-04\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 4.6368e-05 - val_loss: 9.6259e-04\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 4.7537e-05 - val_loss: 9.7795e-04\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 4.3956e-05 - val_loss: 0.0018\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 4.6414e-05 - val_loss: 0.0016\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 4.2559e-05 - val_loss: 0.0012\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 4.1520e-05 - val_loss: 0.0015\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 4.4661e-05 - val_loss: 9.4065e-04\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 4.2237e-05 - val_loss: 8.7369e-04\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 1s 55ms/step - loss: 4.3088e-05 - val_loss: 8.5294e-04\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 4.2706e-05 - val_loss: 9.1201e-04\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 4.3222e-05 - val_loss: 0.0011\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 4.1111e-05 - val_loss: 8.3073e-04\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 1s 53ms/step - loss: 4.3454e-05 - val_loss: 8.5599e-04\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 1s 51ms/step - loss: 3.7627e-05 - val_loss: 8.2506e-04\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 4.0595e-05 - val_loss: 0.0011\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 3.8647e-05 - val_loss: 0.0013\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 3.7220e-05 - val_loss: 8.1528e-04\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 4.3126e-05 - val_loss: 0.0011\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 4.8729e-05 - val_loss: 9.9429e-04\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 4.7635e-05 - val_loss: 0.0037\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 1s 91ms/step - loss: 3.8898e-05 - val_loss: 8.8149e-04\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 3.4205e-05 - val_loss: 7.4979e-04\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 2s 91ms/step - loss: 3.7778e-05 - val_loss: 9.3670e-04\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 3.1605e-05 - val_loss: 0.0014\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 1s 65ms/step - loss: 3.4541e-05 - val_loss: 9.6430e-04\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 3.5432e-05 - val_loss: 0.0014\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 3.2755e-05 - val_loss: 7.0718e-04\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 3.2898e-05 - val_loss: 0.0012\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 3.5801e-05 - val_loss: 6.9333e-04\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 1s 54ms/step - loss: 3.2889e-05 - val_loss: 0.0011\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 3.4038e-05 - val_loss: 7.8612e-04\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 3.2094e-05 - val_loss: 0.0017\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 3.4569e-05 - val_loss: 0.0015\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 1s 69ms/step - loss: 3.0681e-05 - val_loss: 0.0016\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 3.0026e-05 - val_loss: 6.8683e-04\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 3.1550e-05 - val_loss: 8.4309e-04\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 2.9636e-05 - val_loss: 0.0015\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 2.8753e-05 - val_loss: 0.0014\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 3.6724e-05 - val_loss: 0.0037\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 4.0969e-05 - val_loss: 0.0018\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 3.0566e-05 - val_loss: 9.3685e-04\n",
      "16/16 [==============================] - 1s 47ms/step - loss: 2.4789e-04\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 8s 170ms/step - loss: 0.2116 - val_loss: 1.0241\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.2068 - val_loss: 1.0167\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.2055 - val_loss: 1.0091\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.2012 - val_loss: 1.0013\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 1s 95ms/step - loss: 0.2030 - val_loss: 0.9935\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 1s 96ms/step - loss: 0.1979 - val_loss: 0.9857\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.1965 - val_loss: 0.9778\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.1945 - val_loss: 0.9698\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1917 - val_loss: 0.9618\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1912 - val_loss: 0.9537\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1900 - val_loss: 0.9456\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1877 - val_loss: 0.9375\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1863 - val_loss: 0.9293\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1829 - val_loss: 0.9211\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1799 - val_loss: 0.9129\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1776 - val_loss: 0.9048\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1781 - val_loss: 0.8965\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 0s 26ms/step - loss: 0.1764 - val_loss: 0.8881\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 0s 27ms/step - loss: 0.1726 - val_loss: 0.8797\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.1726 - val_loss: 0.8714\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.1687 - val_loss: 0.8630\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.1664 - val_loss: 0.8545\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.1672 - val_loss: 0.8460\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.1663 - val_loss: 0.8373\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 1s 76ms/step - loss: 0.1614 - val_loss: 0.8289\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.1586 - val_loss: 0.8202\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.1581 - val_loss: 0.8116\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.1557 - val_loss: 0.8031\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.1535 - val_loss: 0.7945\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.1521 - val_loss: 0.7859\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.1497 - val_loss: 0.7773\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.1477 - val_loss: 0.7687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.1431 - val_loss: 0.7602\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 2s 91ms/step - loss: 0.1428 - val_loss: 0.7517\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.1427 - val_loss: 0.7431\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 1s 56ms/step - loss: 0.1397 - val_loss: 0.7346\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.1358 - val_loss: 0.7259\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.1365 - val_loss: 0.7173\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.1340 - val_loss: 0.7087\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.1312 - val_loss: 0.7001\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.1261 - val_loss: 0.6915\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.1264 - val_loss: 0.6830\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.1246 - val_loss: 0.6744\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.1246 - val_loss: 0.6657\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.1229 - val_loss: 0.6572\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.1191 - val_loss: 0.6487\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.1178 - val_loss: 0.6402\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.1158 - val_loss: 0.6317\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.1143 - val_loss: 0.6234\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.1093 - val_loss: 0.6149\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.1084 - val_loss: 0.6065\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.1054 - val_loss: 0.5982\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.1048 - val_loss: 0.5899\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.1053 - val_loss: 0.5815\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.1025 - val_loss: 0.5732\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.1000 - val_loss: 0.5647\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 1s 58ms/step - loss: 0.0971 - val_loss: 0.5566\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 1s 72ms/step - loss: 0.0960 - val_loss: 0.5484\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0932 - val_loss: 0.5402\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0928 - val_loss: 0.5321\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 1s 68ms/step - loss: 0.0901 - val_loss: 0.5239\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0895 - val_loss: 0.5157\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 1s 95ms/step - loss: 0.0891 - val_loss: 0.5076\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0835 - val_loss: 0.4996\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 1s 75ms/step - loss: 0.0826 - val_loss: 0.4916\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0824 - val_loss: 0.4837\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0806 - val_loss: 0.4758\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.0768 - val_loss: 0.4679\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0767 - val_loss: 0.4601\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0740 - val_loss: 0.4525\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0725 - val_loss: 0.4449\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.0708 - val_loss: 0.4372\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0708 - val_loss: 0.4297\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0675 - val_loss: 0.4221\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0673 - val_loss: 0.4148\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0650 - val_loss: 0.4074\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 1s 78ms/step - loss: 0.0641 - val_loss: 0.4002\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0620 - val_loss: 0.3930\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.0584 - val_loss: 0.3859\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0594 - val_loss: 0.3789\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0601 - val_loss: 0.3718\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0560 - val_loss: 0.3649\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0540 - val_loss: 0.3580\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0537 - val_loss: 0.3512\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0529 - val_loss: 0.3445\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0499 - val_loss: 0.3379\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0500 - val_loss: 0.3312\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 2s 101ms/step - loss: 0.0458 - val_loss: 0.3249\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 2s 112ms/step - loss: 0.0483 - val_loss: 0.3185\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 2s 108ms/step - loss: 0.0454 - val_loss: 0.3122\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0457 - val_loss: 0.3058\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0431 - val_loss: 0.2997\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0435 - val_loss: 0.2935\n",
      "Epoch 94/100\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.0409 - val_loss: 0.2875\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 2s 103ms/step - loss: 0.0403 - val_loss: 0.2816\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0391 - val_loss: 0.2757\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0370 - val_loss: 0.2700\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.0371 - val_loss: 0.2644\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 0.0351 - val_loss: 0.2588\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.0340 - val_loss: 0.2534\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 2.9383e-04\n",
      "Epoch 1/100\n",
      "16/16 [==============================] - 7s 159ms/step - loss: 0.0091 - val_loss: 0.9602\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0091 - val_loss: 0.9555\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0088 - val_loss: 0.9509\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0087 - val_loss: 0.9463\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0086 - val_loss: 0.9416\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0084 - val_loss: 0.9369\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 2s 90ms/step - loss: 0.0084 - val_loss: 0.9323\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.0082 - val_loss: 0.9276\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0080 - val_loss: 0.9230\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0079 - val_loss: 0.9184\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0078 - val_loss: 0.9137\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0078 - val_loss: 0.9090\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.0076 - val_loss: 0.9044\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 1s 90ms/step - loss: 0.0074 - val_loss: 0.8997\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0072 - val_loss: 0.8951\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0072 - val_loss: 0.8905\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0071 - val_loss: 0.8860\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 2s 109ms/step - loss: 0.0069 - val_loss: 0.8814\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 0.0067 - val_loss: 0.8769\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0069 - val_loss: 0.8723\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0066 - val_loss: 0.8678\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 2s 93ms/step - loss: 0.0064 - val_loss: 0.8634\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 1s 92ms/step - loss: 0.0063 - val_loss: 0.8589\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0063 - val_loss: 0.8545\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0060 - val_loss: 0.8501\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0060 - val_loss: 0.8457\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0060 - val_loss: 0.8413\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0058 - val_loss: 0.8370\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.0056 - val_loss: 0.8327\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0054 - val_loss: 0.8285\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 1s 93ms/step - loss: 0.0053 - val_loss: 0.8243\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 2s 88ms/step - loss: 0.0053 - val_loss: 0.8201\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0052 - val_loss: 0.8159\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.0052 - val_loss: 0.8117\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0052 - val_loss: 0.8075\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0050 - val_loss: 0.8034\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0047 - val_loss: 0.7994\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 1s 67ms/step - loss: 0.0047 - val_loss: 0.7953\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0047 - val_loss: 0.7913\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 1s 60ms/step - loss: 0.0045 - val_loss: 0.7874\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 1s 59ms/step - loss: 0.0045 - val_loss: 0.7835\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 1s 57ms/step - loss: 0.0044 - val_loss: 0.7796\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 1s 52ms/step - loss: 0.0042 - val_loss: 0.7758\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0041 - val_loss: 0.7719\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0040 - val_loss: 0.7681\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 2s 118ms/step - loss: 0.0041 - val_loss: 0.7643\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 2s 111ms/step - loss: 0.0039 - val_loss: 0.7606\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 1s 71ms/step - loss: 0.0038 - val_loss: 0.7569\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0038 - val_loss: 0.7533\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0037 - val_loss: 0.7497\n",
      "Epoch 51/100\n",
      "16/16 [==============================] - 2s 102ms/step - loss: 0.0036 - val_loss: 0.7461\n",
      "Epoch 52/100\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0036 - val_loss: 0.7425\n",
      "Epoch 53/100\n",
      "16/16 [==============================] - 2s 106ms/step - loss: 0.0035 - val_loss: 0.7389\n",
      "Epoch 54/100\n",
      "16/16 [==============================] - 1s 89ms/step - loss: 0.0034 - val_loss: 0.7355\n",
      "Epoch 55/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0033 - val_loss: 0.7321\n",
      "Epoch 56/100\n",
      "16/16 [==============================] - 1s 82ms/step - loss: 0.0033 - val_loss: 0.7287\n",
      "Epoch 57/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0032 - val_loss: 0.7253\n",
      "Epoch 58/100\n",
      "16/16 [==============================] - 1s 85ms/step - loss: 0.0031 - val_loss: 0.7220\n",
      "Epoch 59/100\n",
      "16/16 [==============================] - 1s 80ms/step - loss: 0.0030 - val_loss: 0.7187\n",
      "Epoch 60/100\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0030 - val_loss: 0.7154\n",
      "Epoch 61/100\n",
      "16/16 [==============================] - 2s 92ms/step - loss: 0.0028 - val_loss: 0.7122\n",
      "Epoch 62/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0029 - val_loss: 0.7090\n",
      "Epoch 63/100\n",
      "16/16 [==============================] - 1s 81ms/step - loss: 0.0029 - val_loss: 0.7058\n",
      "Epoch 64/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.0027 - val_loss: 0.7027\n",
      "Epoch 65/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0026 - val_loss: 0.6997\n",
      "Epoch 66/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0026 - val_loss: 0.6967\n",
      "Epoch 67/100\n",
      "16/16 [==============================] - 2s 100ms/step - loss: 0.0025 - val_loss: 0.6937\n",
      "Epoch 68/100\n",
      "16/16 [==============================] - 1s 77ms/step - loss: 0.0025 - val_loss: 0.6907\n",
      "Epoch 69/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0024 - val_loss: 0.6878\n",
      "Epoch 70/100\n",
      "16/16 [==============================] - 2s 91ms/step - loss: 0.0024 - val_loss: 0.6848\n",
      "Epoch 71/100\n",
      "16/16 [==============================] - 2s 105ms/step - loss: 0.0024 - val_loss: 0.6820\n",
      "Epoch 72/100\n",
      "16/16 [==============================] - 1s 94ms/step - loss: 0.0023 - val_loss: 0.6791\n",
      "Epoch 73/100\n",
      "16/16 [==============================] - 2s 96ms/step - loss: 0.0023 - val_loss: 0.6763\n",
      "Epoch 74/100\n",
      "16/16 [==============================] - 2s 114ms/step - loss: 0.0022 - val_loss: 0.6736\n",
      "Epoch 75/100\n",
      "16/16 [==============================] - 1s 83ms/step - loss: 0.0022 - val_loss: 0.6709\n",
      "Epoch 76/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0021 - val_loss: 0.6682\n",
      "Epoch 77/100\n",
      "16/16 [==============================] - 2s 113ms/step - loss: 0.0021 - val_loss: 0.6656\n",
      "Epoch 78/100\n",
      "16/16 [==============================] - 2s 94ms/step - loss: 0.0021 - val_loss: 0.6630\n",
      "Epoch 79/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0020 - val_loss: 0.6604\n",
      "Epoch 80/100\n",
      "16/16 [==============================] - 2s 110ms/step - loss: 0.0019 - val_loss: 0.6578\n",
      "Epoch 81/100\n",
      "16/16 [==============================] - 1s 87ms/step - loss: 0.0020 - val_loss: 0.6553\n",
      "Epoch 82/100\n",
      "16/16 [==============================] - 2s 90ms/step - loss: 0.0019 - val_loss: 0.6528\n",
      "Epoch 83/100\n",
      "16/16 [==============================] - 1s 88ms/step - loss: 0.0020 - val_loss: 0.6502\n",
      "Epoch 84/100\n",
      "16/16 [==============================] - 2s 107ms/step - loss: 0.0018 - val_loss: 0.6478\n",
      "Epoch 85/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0019 - val_loss: 0.6453\n",
      "Epoch 86/100\n",
      "16/16 [==============================] - 2s 97ms/step - loss: 0.0018 - val_loss: 0.6429\n",
      "Epoch 87/100\n",
      "16/16 [==============================] - 2s 115ms/step - loss: 0.0017 - val_loss: 0.6406\n",
      "Epoch 88/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0017 - val_loss: 0.6383\n",
      "Epoch 89/100\n",
      "16/16 [==============================] - 2s 98ms/step - loss: 0.0016 - val_loss: 0.6362\n",
      "Epoch 90/100\n",
      "16/16 [==============================] - 1s 79ms/step - loss: 0.0016 - val_loss: 0.6339\n",
      "Epoch 91/100\n",
      "16/16 [==============================] - 1s 64ms/step - loss: 0.0016 - val_loss: 0.6317\n",
      "Epoch 92/100\n",
      "16/16 [==============================] - 1s 86ms/step - loss: 0.0016 - val_loss: 0.6296\n",
      "Epoch 93/100\n",
      "16/16 [==============================] - 1s 66ms/step - loss: 0.0016 - val_loss: 0.6275\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 73ms/step - loss: 0.0016 - val_loss: 0.6254\n",
      "Epoch 95/100\n",
      "16/16 [==============================] - 1s 61ms/step - loss: 0.0015 - val_loss: 0.6234\n",
      "Epoch 96/100\n",
      "16/16 [==============================] - 1s 74ms/step - loss: 0.0015 - val_loss: 0.6215\n",
      "Epoch 97/100\n",
      "16/16 [==============================] - 2s 104ms/step - loss: 0.0015 - val_loss: 0.6195\n",
      "Epoch 98/100\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.0015 - val_loss: 0.6175\n",
      "Epoch 99/100\n",
      "16/16 [==============================] - 2s 99ms/step - loss: 0.0014 - val_loss: 0.6156\n",
      "Epoch 100/100\n",
      "16/16 [==============================] - 1s 84ms/step - loss: 0.0015 - val_loss: 0.6137\n",
      "16/16 [==============================] - 1s 70ms/step - loss: 0.0960\n",
      "Epoch 1/100\n",
      "62/62 [==============================] - 9s 77ms/step - loss: 0.0193 - val_loss: 0.0038\n",
      "Epoch 2/100\n",
      "62/62 [==============================] - 3s 50ms/step - loss: 0.0019 - val_loss: 0.0041\n",
      "Epoch 3/100\n",
      "62/62 [==============================] - 3s 52ms/step - loss: 0.0015 - val_loss: 0.0092\n",
      "Epoch 4/100\n",
      "62/62 [==============================] - 3s 51ms/step - loss: 0.0013 - val_loss: 0.0057\n",
      "Epoch 5/100\n",
      "62/62 [==============================] - 3s 48ms/step - loss: 0.0012 - val_loss: 0.0060\n",
      "Epoch 6/100\n",
      "62/62 [==============================] - 2s 33ms/step - loss: 0.0011 - val_loss: 0.0080\n",
      "Epoch 7/100\n",
      "62/62 [==============================] - 2s 34ms/step - loss: 0.0012 - val_loss: 0.0023\n",
      "Epoch 8/100\n",
      "62/62 [==============================] - 3s 42ms/step - loss: 9.7924e-04 - val_loss: 0.0022\n",
      "Epoch 9/100\n",
      "62/62 [==============================] - 3s 44ms/step - loss: 0.0012 - val_loss: 0.0131\n",
      "Epoch 10/100\n",
      "62/62 [==============================] - 2s 36ms/step - loss: 0.0013 - val_loss: 0.0088\n",
      "Epoch 11/100\n",
      "62/62 [==============================] - 3s 48ms/step - loss: 0.0011 - val_loss: 0.0032\n",
      "Epoch 12/100\n",
      "62/62 [==============================] - 3s 45ms/step - loss: 9.1845e-04 - val_loss: 0.0025\n",
      "Epoch 13/100\n",
      "62/62 [==============================] - 3s 42ms/step - loss: 0.0010 - val_loss: 0.0062\n",
      "Epoch 14/100\n",
      "62/62 [==============================] - 3s 47ms/step - loss: 9.7395e-04 - val_loss: 0.0074\n",
      "Epoch 15/100\n",
      "62/62 [==============================] - 3s 45ms/step - loss: 0.0012 - val_loss: 0.0018\n",
      "Epoch 16/100\n",
      "62/62 [==============================] - 3s 48ms/step - loss: 8.9721e-04 - val_loss: 0.0062\n",
      "Epoch 17/100\n",
      "62/62 [==============================] - 3s 41ms/step - loss: 9.8117e-04 - val_loss: 0.0076\n",
      "Epoch 18/100\n",
      "62/62 [==============================] - 3s 41ms/step - loss: 9.0868e-04 - val_loss: 0.0016\n",
      "Epoch 19/100\n",
      "62/62 [==============================] - 4s 58ms/step - loss: 8.3193e-04 - val_loss: 0.0019\n",
      "Epoch 20/100\n",
      "62/62 [==============================] - 3s 52ms/step - loss: 8.3963e-04 - val_loss: 0.0039\n",
      "Epoch 21/100\n",
      "62/62 [==============================] - 3s 50ms/step - loss: 9.0041e-04 - val_loss: 0.0079\n",
      "Epoch 22/100\n",
      "62/62 [==============================] - 3s 53ms/step - loss: 8.5427e-04 - val_loss: 0.0022\n",
      "Epoch 23/100\n",
      "62/62 [==============================] - 3s 43ms/step - loss: 7.5896e-04 - val_loss: 0.0050\n",
      "Epoch 24/100\n",
      "62/62 [==============================] - 3s 44ms/step - loss: 7.9027e-04 - val_loss: 0.0012\n",
      "Epoch 25/100\n",
      "62/62 [==============================] - 3s 51ms/step - loss: 8.4586e-04 - val_loss: 0.0011\n",
      "Epoch 26/100\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 7.3483e-04 - val_loss: 0.0014\n",
      "Epoch 27/100\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 7.3395e-04 - val_loss: 0.0026\n",
      "Epoch 28/100\n",
      "62/62 [==============================] - 3s 50ms/step - loss: 6.6342e-04 - val_loss: 0.0031\n",
      "Epoch 29/100\n",
      "62/62 [==============================] - 3s 48ms/step - loss: 7.2212e-04 - val_loss: 0.0013\n",
      "Epoch 30/100\n",
      "62/62 [==============================] - 2s 34ms/step - loss: 6.9272e-04 - val_loss: 0.0011\n",
      "Epoch 31/100\n",
      "62/62 [==============================] - 3s 44ms/step - loss: 7.8016e-04 - val_loss: 0.0011\n",
      "Epoch 32/100\n",
      "62/62 [==============================] - 3s 43ms/step - loss: 6.3754e-04 - val_loss: 0.0036\n",
      "Epoch 33/100\n",
      "62/62 [==============================] - 2s 40ms/step - loss: 7.2209e-04 - val_loss: 0.0012\n",
      "Epoch 34/100\n",
      "62/62 [==============================] - 3s 47ms/step - loss: 6.6593e-04 - val_loss: 0.0034\n",
      "Epoch 35/100\n",
      "62/62 [==============================] - 3s 43ms/step - loss: 6.3782e-04 - val_loss: 0.0020\n",
      "Epoch 36/100\n",
      "62/62 [==============================] - 3s 42ms/step - loss: 7.8885e-04 - val_loss: 0.0012\n",
      "Epoch 37/100\n",
      "62/62 [==============================] - 3s 43ms/step - loss: 6.1392e-04 - val_loss: 9.5801e-04\n",
      "Epoch 38/100\n",
      "62/62 [==============================] - 3s 47ms/step - loss: 5.2398e-04 - val_loss: 0.0033\n",
      "Epoch 39/100\n",
      "62/62 [==============================] - 3s 44ms/step - loss: 6.3866e-04 - val_loss: 0.0032\n",
      "Epoch 40/100\n",
      "62/62 [==============================] - 2s 38ms/step - loss: 6.5200e-04 - val_loss: 0.0017\n",
      "Epoch 41/100\n",
      "62/62 [==============================] - 3s 47ms/step - loss: 5.7027e-04 - val_loss: 9.0190e-04\n",
      "Epoch 42/100\n",
      "62/62 [==============================] - 3s 53ms/step - loss: 5.6653e-04 - val_loss: 0.0030\n",
      "Epoch 43/100\n",
      "62/62 [==============================] - 3s 49ms/step - loss: 6.0719e-04 - val_loss: 0.0015\n",
      "Epoch 44/100\n",
      "62/62 [==============================] - 3s 42ms/step - loss: 5.1237e-04 - val_loss: 8.4312e-04\n",
      "Epoch 45/100\n",
      "62/62 [==============================] - 3s 40ms/step - loss: 5.0342e-04 - val_loss: 0.0014\n",
      "Epoch 46/100\n",
      "62/62 [==============================] - 2s 37ms/step - loss: 7.3997e-04 - val_loss: 0.0014\n",
      "Epoch 47/100\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 5.8752e-04 - val_loss: 0.0015\n",
      "Epoch 48/100\n",
      "62/62 [==============================] - 2s 31ms/step - loss: 5.3116e-04 - val_loss: 0.0017\n",
      "Epoch 49/100\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 6.6157e-04 - val_loss: 8.2257e-04\n",
      "Epoch 50/100\n",
      "62/62 [==============================] - 2s 30ms/step - loss: 5.8020e-04 - val_loss: 7.5884e-04\n",
      "Epoch 51/100\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 5.8291e-04 - val_loss: 8.0216e-04\n",
      "Epoch 52/100\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 5.4655e-04 - val_loss: 0.0014\n",
      "Epoch 53/100\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 4.9296e-04 - val_loss: 0.0022\n",
      "Epoch 54/100\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 5.8163e-04 - val_loss: 7.9555e-04\n",
      "Epoch 55/100\n",
      "62/62 [==============================] - 2s 29ms/step - loss: 4.7019e-04 - val_loss: 0.0028\n",
      "Epoch 56/100\n",
      "62/62 [==============================] - 2s 28ms/step - loss: 5.3402e-04 - val_loss: 7.8745e-04\n",
      "Epoch 57/100\n",
      "62/62 [==============================] - 3s 56ms/step - loss: 5.4408e-04 - val_loss: 8.2314e-04\n",
      "Epoch 58/100\n",
      "62/62 [==============================] - 4s 57ms/step - loss: 4.5983e-04 - val_loss: 6.8657e-04\n",
      "Epoch 59/100\n",
      "62/62 [==============================] - 3s 53ms/step - loss: 5.2063e-04 - val_loss: 0.0026\n",
      "Epoch 60/100\n",
      "62/62 [==============================] - 3s 55ms/step - loss: 5.4091e-04 - val_loss: 9.4315e-04\n",
      "Epoch 61/100\n",
      "62/62 [==============================] - 5s 86ms/step - loss: 4.6776e-04 - val_loss: 0.0019\n",
      "Epoch 62/100\n",
      "62/62 [==============================] - 5s 81ms/step - loss: 5.0856e-04 - val_loss: 0.0052\n",
      "Epoch 63/100\n",
      "62/62 [==============================] - 5s 76ms/step - loss: 4.7518e-04 - val_loss: 0.0038\n",
      "Epoch 64/100\n",
      "62/62 [==============================] - 4s 68ms/step - loss: 5.5518e-04 - val_loss: 6.3297e-04\n",
      "Epoch 65/100\n",
      "62/62 [==============================] - 5s 83ms/step - loss: 4.8334e-04 - val_loss: 0.0021\n",
      "Epoch 66/100\n",
      "62/62 [==============================] - 3s 53ms/step - loss: 4.9900e-04 - val_loss: 0.0053\n",
      "Epoch 67/100\n",
      "62/62 [==============================] - 4s 63ms/step - loss: 4.8457e-04 - val_loss: 0.0014\n",
      "Epoch 68/100\n",
      "62/62 [==============================] - 4s 57ms/step - loss: 5.0743e-04 - val_loss: 0.0012\n",
      "Epoch 69/100\n",
      "62/62 [==============================] - 2s 34ms/step - loss: 4.7071e-04 - val_loss: 0.0025\n",
      "Epoch 70/100\n",
      "62/62 [==============================] - 4s 58ms/step - loss: 5.6738e-04 - val_loss: 0.0029\n",
      "Epoch 71/100\n",
      "62/62 [==============================] - 4s 66ms/step - loss: 4.9126e-04 - val_loss: 0.0017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "62/62 [==============================] - 4s 58ms/step - loss: 5.1572e-04 - val_loss: 0.0022\n",
      "Epoch 73/100\n",
      "62/62 [==============================] - 3s 48ms/step - loss: 5.8104e-04 - val_loss: 6.6905e-04\n",
      "Epoch 74/100\n",
      "62/62 [==============================] - 5s 73ms/step - loss: 4.9667e-04 - val_loss: 0.0060\n",
      "Epoch 75/100\n",
      "62/62 [==============================] - 5s 78ms/step - loss: 5.7942e-04 - val_loss: 0.0032\n",
      "Epoch 76/100\n",
      "62/62 [==============================] - 3s 51ms/step - loss: 4.5330e-04 - val_loss: 9.9500e-04\n",
      "Epoch 77/100\n",
      "62/62 [==============================] - 5s 82ms/step - loss: 5.2690e-04 - val_loss: 0.0011\n",
      "Epoch 78/100\n",
      "62/62 [==============================] - 4s 64ms/step - loss: 4.5363e-04 - val_loss: 0.0025\n",
      "Epoch 79/100\n",
      "62/62 [==============================] - 6s 93ms/step - loss: 5.0081e-04 - val_loss: 0.0027\n",
      "Epoch 80/100\n",
      "62/62 [==============================] - 5s 87ms/step - loss: 4.6492e-04 - val_loss: 8.7069e-04\n",
      "Epoch 81/100\n",
      "62/62 [==============================] - 6s 97ms/step - loss: 5.6307e-04 - val_loss: 5.8911e-04\n",
      "Epoch 82/100\n",
      "62/62 [==============================] - 5s 84ms/step - loss: 4.2188e-04 - val_loss: 5.2520e-04\n",
      "Epoch 83/100\n",
      "62/62 [==============================] - 4s 68ms/step - loss: 4.5759e-04 - val_loss: 0.0024\n",
      "Epoch 84/100\n",
      "62/62 [==============================] - 7s 106ms/step - loss: 4.5653e-04 - val_loss: 0.0011\n",
      "Epoch 85/100\n",
      "62/62 [==============================] - 3s 50ms/step - loss: 4.2239e-04 - val_loss: 0.0012\n",
      "Epoch 86/100\n",
      "62/62 [==============================] - 5s 88ms/step - loss: 4.5222e-04 - val_loss: 8.0485e-04\n",
      "Epoch 87/100\n",
      "62/62 [==============================] - 6s 95ms/step - loss: 4.9396e-04 - val_loss: 0.0016\n",
      "Epoch 88/100\n",
      "62/62 [==============================] - 4s 59ms/step - loss: 5.4953e-04 - val_loss: 5.1208e-04\n",
      "Epoch 89/100\n",
      "62/62 [==============================] - 4s 64ms/step - loss: 5.3610e-04 - val_loss: 7.7859e-04\n",
      "Epoch 90/100\n",
      "62/62 [==============================] - 4s 73ms/step - loss: 4.3730e-04 - val_loss: 6.9947e-04\n",
      "Epoch 91/100\n",
      "62/62 [==============================] - 7s 107ms/step - loss: 4.9019e-04 - val_loss: 0.0017\n",
      "Epoch 92/100\n",
      "62/62 [==============================] - 6s 89ms/step - loss: 4.5860e-04 - val_loss: 0.0037\n",
      "Epoch 93/100\n",
      "62/62 [==============================] - 5s 81ms/step - loss: 4.3257e-04 - val_loss: 8.4266e-04\n",
      "Epoch 94/100\n",
      "62/62 [==============================] - 6s 95ms/step - loss: 3.7406e-04 - val_loss: 0.0028\n",
      "Epoch 95/100\n",
      "62/62 [==============================] - 6s 91ms/step - loss: 4.0005e-04 - val_loss: 4.3063e-04\n",
      "Epoch 96/100\n",
      "62/62 [==============================] - 6s 96ms/step - loss: 4.7966e-04 - val_loss: 0.0026\n",
      "Epoch 97/100\n",
      "62/62 [==============================] - 6s 103ms/step - loss: 4.0864e-04 - val_loss: 5.9257e-04\n",
      "Epoch 98/100\n",
      "62/62 [==============================] - 4s 63ms/step - loss: 3.9399e-04 - val_loss: 0.0010\n",
      "Epoch 99/100\n",
      "62/62 [==============================] - 6s 91ms/step - loss: 4.0685e-04 - val_loss: 4.5081e-04\n",
      "Epoch 100/100\n",
      "62/62 [==============================] - 5s 81ms/step - loss: 4.9366e-04 - val_loss: 0.0023\n"
     ]
    }
   ],
   "source": [
    "grid_search = grid_search.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9e26e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32, 'epochs': 100, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6851ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model=grid_search.best_estimator_.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f0b2e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x2c15344ec70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eeb97d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 2s 29ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction=my_model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8699c0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction\n",
      " [[0.9901055 ]\n",
      " [1.0000424 ]\n",
      " [1.0088454 ]\n",
      " [1.0134742 ]\n",
      " [1.0172639 ]\n",
      " [1.002533  ]\n",
      " [0.99198264]\n",
      " [0.999435  ]\n",
      " [1.009002  ]\n",
      " [1.0144069 ]\n",
      " [1.009774  ]\n",
      " [1.0041944 ]\n",
      " [1.0037242 ]\n",
      " [1.0225303 ]\n",
      " [1.0291309 ]\n",
      " [1.0351877 ]\n",
      " [1.0395119 ]\n",
      " [1.0427263 ]\n",
      " [1.053839  ]\n",
      " [1.0522715 ]\n",
      " [1.047183  ]\n",
      " [1.0228908 ]\n",
      " [1.0138311 ]\n",
      " [1.0058496 ]\n",
      " [1.0041811 ]\n",
      " [1.0049481 ]\n",
      " [0.99553025]\n",
      " [0.97301775]\n",
      " [0.9719804 ]\n",
      " [0.9816457 ]\n",
      " [0.99193543]\n",
      " [0.994428  ]\n",
      " [0.9866402 ]\n",
      " [0.96983314]\n",
      " [0.9683382 ]\n",
      " [0.96436507]\n",
      " [0.9609096 ]\n",
      " [0.94973963]\n",
      " [0.95224756]\n",
      " [0.9542691 ]\n",
      " [0.9686606 ]\n",
      " [0.9715625 ]\n",
      " [0.9699526 ]\n",
      " [0.9640773 ]\n",
      " [0.955366  ]\n",
      " [0.9664332 ]\n",
      " [0.977119  ]\n",
      " [0.984692  ]\n",
      " [1.0011032 ]\n",
      " [1.0097313 ]\n",
      " [1.011008  ]\n",
      " [1.01085   ]\n",
      " [1.0076141 ]\n",
      " [1.0120435 ]\n",
      " [1.0102916 ]\n",
      " [1.023681  ]\n",
      " [1.0100894 ]\n",
      " [1.008494  ]\n",
      " [1.013422  ]\n",
      " [1.0218503 ]\n",
      " [1.0244946 ]\n",
      " [1.0249314 ]\n",
      " [1.0221806 ]\n",
      " [1.0214376 ]\n",
      " [1.0095129 ]\n",
      " [1.005629  ]\n",
      " [1.0103581 ]\n",
      " [1.0173469 ]\n",
      " [1.0206742 ]\n",
      " [1.0333574 ]\n",
      " [1.0542867 ]\n",
      " [1.0749292 ]\n",
      " [1.0921066 ]\n",
      " [1.0882384 ]\n",
      " [1.0894537 ]\n",
      " [1.06867   ]\n",
      " [1.0775435 ]\n",
      " [1.0974264 ]\n",
      " [1.1158812 ]\n",
      " [1.0942795 ]\n",
      " [1.0928531 ]\n",
      " [1.1111634 ]\n",
      " [1.1421719 ]\n",
      " [1.1661966 ]\n",
      " [1.174263  ]\n",
      " [1.1909826 ]\n",
      " [1.1882732 ]\n",
      " [1.1701735 ]\n",
      " [1.1834729 ]\n",
      " [1.1676207 ]\n",
      " [1.1470678 ]\n",
      " [1.1368808 ]\n",
      " [1.1521525 ]\n",
      " [1.1698761 ]\n",
      " [1.1818888 ]\n",
      " [1.2004609 ]\n",
      " [1.2039757 ]\n",
      " [1.2021158 ]\n",
      " [1.1983125 ]\n",
      " [1.1925688 ]\n",
      " [1.2072419 ]\n",
      " [1.207876  ]\n",
      " [1.1830544 ]\n",
      " [1.1594689 ]\n",
      " [1.1565254 ]\n",
      " [1.148588  ]\n",
      " [1.1653378 ]\n",
      " [1.1792655 ]\n",
      " [1.1659806 ]\n",
      " [1.1600125 ]\n",
      " [1.1472607 ]\n",
      " [1.1286516 ]\n",
      " [1.1161602 ]\n",
      " [1.1016192 ]\n",
      " [1.0798582 ]\n",
      " [1.0764096 ]\n",
      " [1.0817534 ]\n",
      " [1.0808377 ]\n",
      " [1.1237907 ]\n",
      " [1.1611125 ]\n",
      " [1.171366  ]\n",
      " [1.1770408 ]\n",
      " [1.1667701 ]\n",
      " [1.1579211 ]\n",
      " [1.1560553 ]\n",
      " [1.1658103 ]\n",
      " [1.1806089 ]\n",
      " [1.1630783 ]\n",
      " [1.1426756 ]\n",
      " [1.1327007 ]\n",
      " [1.1541114 ]\n",
      " [1.157478  ]\n",
      " [1.1437136 ]\n",
      " [1.1318235 ]\n",
      " [1.1099277 ]\n",
      " [1.0924554 ]\n",
      " [1.0739768 ]\n",
      " [1.1011354 ]\n",
      " [1.1085498 ]\n",
      " [1.1062293 ]\n",
      " [1.1161616 ]\n",
      " [1.1245961 ]\n",
      " [1.1063342 ]\n",
      " [1.0872424 ]\n",
      " [1.0691965 ]\n",
      " [1.0892922 ]\n",
      " [1.0731623 ]\n",
      " [1.0567513 ]\n",
      " [1.0278162 ]\n",
      " [1.0359747 ]\n",
      " [1.064029  ]\n",
      " [1.0793726 ]\n",
      " [1.0970104 ]\n",
      " [1.11189   ]\n",
      " [1.129385  ]\n",
      " [1.1429784 ]\n",
      " [1.1613936 ]\n",
      " [1.1712623 ]\n",
      " [1.1716967 ]\n",
      " [1.1925783 ]\n",
      " [1.1932244 ]\n",
      " [1.178196  ]\n",
      " [1.16692   ]\n",
      " [1.1848643 ]\n",
      " [1.1800487 ]\n",
      " [1.1563301 ]\n",
      " [1.1536293 ]\n",
      " [1.1465037 ]\n",
      " [1.1246693 ]\n",
      " [1.1290748 ]\n",
      " [1.1389453 ]\n",
      " [1.1268024 ]\n",
      " [1.1136775 ]\n",
      " [1.120914  ]\n",
      " [1.1284915 ]\n",
      " [1.1290114 ]\n",
      " [1.1052395 ]\n",
      " [1.0921237 ]\n",
      " [1.0724003 ]\n",
      " [1.0606753 ]\n",
      " [1.0888994 ]\n",
      " [1.0795255 ]\n",
      " [1.0620251 ]\n",
      " [1.0712235 ]\n",
      " [1.1005557 ]\n",
      " [1.0749297 ]\n",
      " [1.061031  ]\n",
      " [1.037627  ]\n",
      " [1.0431225 ]\n",
      " [1.01141   ]\n",
      " [0.97013634]\n",
      " [0.9830501 ]\n",
      " [0.98591083]\n",
      " [1.0041175 ]\n",
      " [0.97384024]\n",
      " [0.94337493]\n",
      " [0.92989993]\n",
      " [0.94960034]\n",
      " [0.9499134 ]\n",
      " [0.95008117]\n",
      " [0.95830274]\n",
      " [0.9964765 ]\n",
      " [1.0082455 ]\n",
      " [1.0122161 ]\n",
      " [1.0154961 ]\n",
      " [0.9938342 ]\n",
      " [0.99176097]\n",
      " [0.9958216 ]\n",
      " [1.0039312 ]\n",
      " [0.98232734]\n",
      " [0.94510794]\n",
      " [0.9062853 ]\n",
      " [0.898463  ]\n",
      " [0.90869784]\n",
      " [0.8901049 ]\n",
      " [0.88827705]\n",
      " [0.9100537 ]\n",
      " [0.9179143 ]\n",
      " [0.93104035]\n",
      " [0.95284164]\n",
      " [0.9630484 ]\n",
      " [0.94837457]\n",
      " [0.9418722 ]\n",
      " [0.9276404 ]\n",
      " [0.93226945]\n",
      " [0.94577366]\n",
      " [0.9632579 ]\n",
      " [0.9826647 ]\n",
      " [0.99318296]\n",
      " [0.9869579 ]\n",
      " [0.99024856]\n",
      " [0.9824698 ]\n",
      " [0.992492  ]\n",
      " [1.0116758 ]\n",
      " [1.0065426 ]\n",
      " [1.012908  ]\n",
      " [1.0292902 ]\n",
      " [1.0444517 ]\n",
      " [1.0469131 ]\n",
      " [1.0403901 ]\n",
      " [1.0303192 ]\n",
      " [1.047276  ]\n",
      " [1.0588741 ]\n",
      " [1.0893358 ]\n",
      " [1.0940415 ]\n",
      " [1.0866085 ]\n",
      " [1.1063535 ]\n",
      " [1.1184623 ]\n",
      " [1.113404  ]\n",
      " [1.1164804 ]\n",
      " [1.1119014 ]\n",
      " [1.133594  ]\n",
      " [1.1398587 ]\n",
      " [1.1523192 ]\n",
      " [1.1621072 ]\n",
      " [1.1638341 ]\n",
      " [1.170711  ]\n",
      " [1.1707801 ]\n",
      " [1.1589003 ]\n",
      " [1.1349444 ]\n",
      " [1.1281469 ]\n",
      " [1.1279218 ]\n",
      " [1.1412034 ]\n",
      " [1.1205897 ]\n",
      " [1.0928679 ]\n",
      " [1.0800791 ]\n",
      " [1.070163  ]\n",
      " [1.0636743 ]\n",
      " [1.0618304 ]\n",
      " [1.051651  ]\n",
      " [1.052139  ]\n",
      " [1.0472686 ]\n",
      " [1.059062  ]\n",
      " [1.0918187 ]\n",
      " [1.059939  ]\n",
      " [1.051417  ]\n",
      " [1.0375701 ]\n",
      " [1.019551  ]\n",
      " [1.0304182 ]\n",
      " [1.0508475 ]\n",
      " [1.0505838 ]\n",
      " [1.0366794 ]\n",
      " [1.0210335 ]\n",
      " [1.0214863 ]\n",
      " [1.0280077 ]\n",
      " [1.0094386 ]\n",
      " [0.9773378 ]\n",
      " [0.94952637]\n",
      " [0.9518368 ]\n",
      " [0.9798341 ]\n",
      " [0.9869682 ]\n",
      " [0.9904782 ]\n",
      " [0.9631064 ]\n",
      " [0.95331955]\n",
      " [0.9464491 ]\n",
      " [0.94229394]\n",
      " [0.9476218 ]\n",
      " [0.95047176]\n",
      " [0.9599794 ]\n",
      " [0.9722385 ]\n",
      " [0.973199  ]\n",
      " [0.9752023 ]\n",
      " [0.9864037 ]\n",
      " [1.0043559 ]\n",
      " [1.0239466 ]\n",
      " [1.0176404 ]\n",
      " [0.9932988 ]\n",
      " [1.0273211 ]\n",
      " [1.0367334 ]\n",
      " [1.029169  ]\n",
      " [0.9996655 ]\n",
      " [0.9570239 ]\n",
      " [0.93943965]\n",
      " [0.9344335 ]\n",
      " [0.94250935]\n",
      " [0.92682344]\n",
      " [0.96744657]\n",
      " [0.9994164 ]\n",
      " [1.0093397 ]\n",
      " [1.0205841 ]\n",
      " [1.0119984 ]\n",
      " [1.0122354 ]\n",
      " [1.0240432 ]\n",
      " [1.0113995 ]\n",
      " [1.0116885 ]\n",
      " [1.019912  ]\n",
      " [1.0083543 ]\n",
      " [0.98607594]\n",
      " [0.9668941 ]\n",
      " [0.98142713]\n",
      " [1.0009882 ]\n",
      " [1.000413  ]\n",
      " [1.0007107 ]\n",
      " [0.9818644 ]\n",
      " [0.9630013 ]\n",
      " [0.96502984]\n",
      " [0.96642363]\n",
      " [0.9726433 ]\n",
      " [0.98980904]\n",
      " [0.9781208 ]\n",
      " [0.9440771 ]\n",
      " [0.91993487]\n",
      " [0.90421134]\n",
      " [0.89466447]\n",
      " [0.9084779 ]\n",
      " [0.9014951 ]\n",
      " [0.8937979 ]\n",
      " [0.8854402 ]\n",
      " [0.86809343]\n",
      " [0.8725172 ]\n",
      " [0.87512016]\n",
      " [0.8614515 ]\n",
      " [0.85740423]\n",
      " [0.8525328 ]\n",
      " [0.86363   ]\n",
      " [0.8814023 ]\n",
      " [0.8838132 ]\n",
      " [0.8961065 ]\n",
      " [0.9028072 ]\n",
      " [0.90735734]\n",
      " [0.91821015]\n",
      " [0.9225096 ]\n",
      " [0.91804236]\n",
      " [0.9254324 ]\n",
      " [0.9470696 ]\n",
      " [0.9611235 ]\n",
      " [0.9599365 ]\n",
      " [0.97154987]\n",
      " [0.98308307]\n",
      " [0.9775708 ]\n",
      " [0.9754608 ]\n",
      " [0.9791008 ]\n",
      " [1.009885  ]\n",
      " [1.030304  ]\n",
      " [1.0316267 ]\n",
      " [1.0382009 ]\n",
      " [1.0346704 ]\n",
      " [1.0292097 ]\n",
      " [1.0216012 ]\n",
      " [1.0337999 ]\n",
      " [1.0353595 ]\n",
      " [1.0461144 ]\n",
      " [1.0457048 ]\n",
      " [1.0351474 ]\n",
      " [1.0161386 ]\n",
      " [1.0096478 ]\n",
      " [1.0113537 ]\n",
      " [0.99859697]\n",
      " [1.0039382 ]\n",
      " [1.0029323 ]\n",
      " [0.9931474 ]\n",
      " [0.98892325]\n",
      " [1.0120492 ]\n",
      " [1.0415744 ]\n",
      " [1.0362766 ]\n",
      " [1.03736   ]\n",
      " [1.030233  ]\n",
      " [1.0137136 ]\n",
      " [1.0157375 ]\n",
      " [1.0283577 ]\n",
      " [1.0316787 ]\n",
      " [1.0456296 ]\n",
      " [1.0521151 ]\n",
      " [1.0594907 ]\n",
      " [1.0717719 ]\n",
      " [1.0752695 ]\n",
      " [1.0771667 ]\n",
      " [1.0800076 ]\n",
      " [1.0764318 ]\n",
      " [1.0679693 ]\n",
      " [1.082824  ]\n",
      " [1.0955379 ]\n",
      " [1.1076853 ]\n",
      " [1.1186682 ]\n",
      " [1.1217467 ]\n",
      " [1.1086293 ]\n",
      " [1.1076303 ]\n",
      " [1.0947909 ]\n",
      " [1.0911422 ]\n",
      " [1.0868633 ]\n",
      " [1.1066909 ]\n",
      " [1.1158016 ]\n",
      " [1.1167047 ]\n",
      " [1.1244045 ]\n",
      " [1.1287119 ]\n",
      " [1.1263319 ]\n",
      " [1.1176838 ]\n",
      " [1.1157082 ]\n",
      " [1.111803  ]\n",
      " [1.1077477 ]\n",
      " [1.1273465 ]\n",
      " [1.1413796 ]\n",
      " [1.1447897 ]\n",
      " [1.1405939 ]\n",
      " [1.1357766 ]\n",
      " [1.1196485 ]\n",
      " [1.1580186 ]\n",
      " [1.1667213 ]\n",
      " [1.1619028 ]\n",
      " [1.1661717 ]\n",
      " [1.1685557 ]\n",
      " [1.16341   ]\n",
      " [1.161135  ]\n",
      " [1.160651  ]\n",
      " [1.1597838 ]\n",
      " [1.1722664 ]\n",
      " [1.181001  ]\n",
      " [1.1738086 ]\n",
      " [1.1605804 ]\n",
      " [1.1565337 ]\n",
      " [1.1633143 ]\n",
      " [1.1754427 ]\n",
      " [1.1924561 ]\n",
      " [1.1935178 ]\n",
      " [1.2024487 ]\n",
      " [1.2124989 ]\n",
      " [1.2096573 ]\n",
      " [1.20169   ]\n",
      " [1.1972005 ]\n",
      " [1.2042998 ]\n",
      " [1.2149235 ]\n",
      " [1.2252014 ]\n",
      " [1.2281116 ]\n",
      " [1.2294381 ]\n",
      " [1.2397336 ]\n",
      " [1.2393712 ]\n",
      " [1.2379264 ]\n",
      " [1.2317824 ]\n",
      " [1.2420623 ]\n",
      " [1.245991  ]\n",
      " [1.2438486 ]\n",
      " [1.2513609 ]\n",
      " [1.2609403 ]\n",
      " [1.265208  ]]\n",
      "\n",
      "Prediction Shape- (474, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"prediction\\n\", prediction)\n",
    "print(\"\\nPrediction Shape-\",prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8c654a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler.inverse_transform(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3a731d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_copies = np.repeat(prediction, 6, axis=-1)\n",
    "inverse_transformed_data = scaler.inverse_transform(prediction_copies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44862c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_copies_array = np.repeat(prediction, 6, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dbd500f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(474, 6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_copies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b00bd6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9901055, 0.9901055, 0.9901055, 0.9901055, 0.9901055, 0.9901055],\n",
       "       [1.0000424, 1.0000424, 1.0000424, 1.0000424, 1.0000424, 1.0000424],\n",
       "       [1.0088454, 1.0088454, 1.0088454, 1.0088454, 1.0088454, 1.0088454],\n",
       "       ...,\n",
       "       [1.2513609, 1.2513609, 1.2513609, 1.2513609, 1.2513609, 1.2513609],\n",
       "       [1.2609403, 1.2609403, 1.2609403, 1.2609403, 1.2609403, 1.2609403],\n",
       "       [1.265208 , 1.265208 , 1.265208 , 1.265208 , 1.265208 , 1.265208 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf3108aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=scaler.inverse_transform(prediction_copies)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cfe10827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([142.32146 , 143.60548 , 144.743   , 145.34113 , 145.83081 ,\n",
       "       143.9273  , 142.56401 , 143.527   , 144.76323 , 145.46164 ,\n",
       "       144.86298 , 144.142   , 144.08124 , 146.51134 , 147.36426 ,\n",
       "       148.1469  , 148.70567 , 149.12102 , 150.55698 , 150.35443 ,\n",
       "       149.69691 , 146.55792 , 145.38724 , 144.35588 , 144.14029 ,\n",
       "       144.2394  , 143.02243 , 140.1134  , 139.97935 , 141.2283  ,\n",
       "       142.5579  , 142.87999 , 141.87367 , 139.7019  , 139.50871 ,\n",
       "       138.99532 , 138.54881 , 137.10544 , 137.42952 , 137.69073 ,\n",
       "       139.55038 , 139.92537 , 139.71733 , 138.95813 , 137.83249 ,\n",
       "       139.26256 , 140.64336 , 141.62193 , 143.74255 , 144.85747 ,\n",
       "       145.02245 , 145.00201 , 144.5839  , 145.15625 , 144.92987 ,\n",
       "       146.66003 , 144.90375 , 144.69759 , 145.33438 , 146.42348 ,\n",
       "       146.76517 , 146.8216  , 146.46614 , 146.37015 , 144.82925 ,\n",
       "       144.32736 , 144.93846 , 145.84154 , 146.2715  , 147.91039 ,\n",
       "       150.61484 , 153.28223 , 155.50186 , 155.00201 , 155.15906 ,\n",
       "       152.47342 , 153.62004 , 156.18927 , 158.57397 , 155.78264 ,\n",
       "       155.59831 , 157.96434 , 161.9712  , 165.07564 , 166.11797 ,\n",
       "       168.27844 , 167.92833 , 165.58952 , 167.30804 , 165.25964 ,\n",
       "       162.60385 , 161.28749 , 163.26088 , 165.55109 , 167.10336 ,\n",
       "       169.5032  , 169.95738 , 169.71704 , 169.2256  , 168.4834  ,\n",
       "       170.37944 , 170.46136 , 167.25398 , 164.2063  , 163.82594 ,\n",
       "       162.80028 , 164.96466 , 166.76437 , 165.04771 , 164.27654 ,\n",
       "       162.62877 , 160.22414 , 158.61002 , 156.73106 , 153.91914 ,\n",
       "       153.47351 , 154.16403 , 154.04572 , 159.59602 , 164.41869 ,\n",
       "       165.7436  , 166.4769  , 165.14973 , 164.00629 , 163.7652  ,\n",
       "       165.02573 , 166.93796 , 164.6727  , 162.0363  , 160.74734 ,\n",
       "       163.514   , 163.94904 , 162.17043 , 160.634   , 157.80466 ,\n",
       "       155.54692 , 153.15915 , 156.66853 , 157.62662 , 157.32677 ,\n",
       "       158.6102  , 159.70009 , 157.34032 , 154.8733  , 152.54144 ,\n",
       "       155.13818 , 153.05391 , 150.9333  , 147.19437 , 148.2486  ,\n",
       "       151.87372 , 153.8564  , 156.13551 , 158.05823 , 160.31891 ,\n",
       "       162.07542 , 164.455   , 165.73021 , 165.78635 , 168.48463 ,\n",
       "       168.56813 , 166.62617 , 165.1691  , 167.48784 , 166.86559 ,\n",
       "       163.8007  , 163.4517  , 162.53094 , 159.70955 , 160.27882 ,\n",
       "       161.55428 , 159.9852  , 158.28922 , 159.22429 , 160.20345 ,\n",
       "       160.27063 , 157.19887 , 155.50407 , 152.95544 , 151.44035 ,\n",
       "       155.08743 , 153.87614 , 151.61478 , 152.80338 , 156.59363 ,\n",
       "       153.28229 , 151.48633 , 148.4621  , 149.17223 , 145.07439 ,\n",
       "       139.74107 , 141.40977 , 141.77942 , 144.13206 , 140.2197  ,\n",
       "       136.283   , 134.5418  , 137.08745 , 137.1279  , 137.14958 ,\n",
       "       138.21196 , 143.1447  , 144.66547 , 145.17854 , 145.60239 ,\n",
       "       142.80327 , 142.53537 , 143.06007 , 144.10799 , 141.31638 ,\n",
       "       136.50696 , 131.49036 , 130.47958 , 131.80211 , 129.39955 ,\n",
       "       129.16336 , 131.9773  , 132.99304 , 134.68915 , 137.50629 ,\n",
       "       138.82518 , 136.92905 , 136.08882 , 134.24982 , 134.84799 ,\n",
       "       136.59297 , 138.85226 , 141.35997 , 142.71912 , 141.91473 ,\n",
       "       142.33995 , 141.33478 , 142.62984 , 145.10873 , 144.44542 ,\n",
       "       145.26796 , 147.38483 , 149.34398 , 149.66203 , 148.81915 ,\n",
       "       147.5178  , 149.70892 , 151.20761 , 155.14381 , 155.75188 ,\n",
       "       154.79141 , 157.34282 , 158.9075  , 158.25388 , 158.6514  ,\n",
       "       158.05971 , 160.8628  , 161.6723  , 163.28242 , 164.54721 ,\n",
       "       164.77036 , 165.65898 , 165.66791 , 164.13281 , 161.0373  ,\n",
       "       160.15892 , 160.12984 , 161.84605 , 159.1824  , 155.60022 ,\n",
       "       153.94768 , 152.66635 , 151.8279  , 151.58961 , 150.27426 ,\n",
       "       150.33733 , 149.70798 , 151.23189 , 155.46466 , 151.34521 ,\n",
       "       150.24402 , 148.45476 , 146.12636 , 147.5306  , 150.17044 ,\n",
       "       150.13637 , 148.33965 , 146.31792 , 146.37642 , 147.21912 ,\n",
       "       144.81966 , 140.67163 , 137.07788 , 137.37645 , 140.9942  ,\n",
       "       141.91606 , 142.36961 , 138.83269 , 137.56804 , 136.68025 ,\n",
       "       136.14333 , 136.83179 , 137.20006 , 138.42862 , 140.01271 ,\n",
       "       140.13683 , 140.39569 , 141.84312 , 144.16287 , 146.69435 ,\n",
       "       145.87946 , 142.73409 , 147.13039 , 148.34663 , 147.36917 ,\n",
       "       143.55678 , 138.0467  , 135.7745  , 135.12761 , 136.17116 ,\n",
       "       134.14426 , 139.39351 , 143.52458 , 144.80687 , 146.25984 ,\n",
       "       145.15042 , 145.18105 , 146.70683 , 145.07303 , 145.11037 ,\n",
       "       146.173   , 144.67953 , 141.80075 , 139.32211 , 141.20004 ,\n",
       "       143.7277  , 143.65337 , 143.69185 , 141.25655 , 138.81909 ,\n",
       "       139.08122 , 139.26132 , 140.06502 , 142.28314 , 140.77281 ,\n",
       "       136.37373 , 133.25414 , 131.22237 , 129.98872 , 131.77368 ,\n",
       "       130.87138 , 129.87675 , 128.79678 , 126.555275, 127.12691 ,\n",
       "       127.46326 , 125.697014, 125.174034, 124.544556, 125.978516,\n",
       "       128.27502 , 128.58655 , 130.17506 , 131.04091 , 131.62889 ,\n",
       "       133.03127 , 133.58682 , 133.00958 , 133.96451 , 136.76042 ,\n",
       "       138.57645 , 138.42307 , 139.92374 , 141.41402 , 140.70174 ,\n",
       "       140.4291  , 140.89944 , 144.87732 , 147.51582 , 147.68675 ,\n",
       "       148.53625 , 148.08005 , 147.37444 , 146.39128 , 147.96758 ,\n",
       "       148.1691  , 149.55884 , 149.5059  , 148.1417  , 145.6854  ,\n",
       "       144.84668 , 145.06711 , 143.4187  , 144.10889 , 143.97891 ,\n",
       "       142.71452 , 142.16869 , 145.15698 , 148.97217 , 148.2876  ,\n",
       "       148.4276  , 147.50667 , 145.37206 , 145.63358 , 147.26434 ,\n",
       "       147.69347 , 149.49619 , 150.33423 , 151.2873  , 152.87424 ,\n",
       "       153.32619 , 153.57135 , 153.93845 , 153.47638 , 152.38289 ,\n",
       "       154.30237 , 155.94525 , 157.51491 , 158.9341  , 159.3319  ,\n",
       "       157.6369  , 157.5078  , 155.84872 , 155.37724 , 154.82433 ,\n",
       "       157.38641 , 158.56367 , 158.68037 , 159.67534 , 160.23193 ,\n",
       "       159.9244  , 158.8069  , 158.55162 , 158.047   , 157.52296 ,\n",
       "       160.0555  , 161.86882 , 162.30948 , 161.7673  , 161.14482 ,\n",
       "       159.06076 , 164.01889 , 165.14345 , 164.5208  , 165.07242 ,\n",
       "       165.38048 , 164.71555 , 164.42159 , 164.35904 , 164.247   ,\n",
       "       165.85995 , 166.98863 , 166.05923 , 164.34991 , 163.82701 ,\n",
       "       164.7032  , 166.2704  , 168.46884 , 168.60603 , 169.76007 ,\n",
       "       171.05873 , 170.69154 , 169.66202 , 169.08191 , 169.99927 ,\n",
       "       171.37204 , 172.70012 , 173.07619 , 173.24759 , 174.57796 ,\n",
       "       174.53113 , 174.34442 , 173.55052 , 174.87888 , 175.38652 ,\n",
       "       175.10968 , 176.08041 , 177.31825 , 177.86972 ], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84f2780a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred=scaler.inverse_transform(np.reshape(prediction_copies,(len(prediction),6)))[:,0]\n",
    "# 这是一个数组切片操作，用于选择重塑并进行逆变换后的数组的第一列\n",
    "# 虽然 prediction_copies_array 有6列，但你只对第一列（索引为0的列）感兴趣"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ec05dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_copies_array = np.repeat(testY,6, axis=-1)\n",
    "\n",
    "original_copies_array.shape\n",
    "\n",
    "original=scaler.inverse_transform(np.reshape(original_copies_array,(len(testY),6)))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "442e0222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([146.190002, 148.970001, 148.539993, 150.229996, 149.800003,\n",
       "       145.029999, 147.440002, 148.309998, 149.449997, 149.809998,\n",
       "       148.350006, 147.479996, 149.      , 152.660004, 152.830002,\n",
       "       153.869995, 153.759995, 154.970001, 156.979996, 155.490005,\n",
       "       155.      , 150.630005, 150.350006, 148.559998, 148.440002,\n",
       "       148.820007, 143.800003, 143.929993, 144.449997, 146.649994,\n",
       "       145.660004, 145.470001, 143.25    , 142.470001, 143.660004,\n",
       "       141.899994, 141.759995, 139.490005, 139.470001, 143.059998,\n",
       "       144.029999, 142.270004, 143.229996, 141.240005, 142.110001,\n",
       "       143.770004, 143.449997, 147.009995, 148.699997, 148.809998,\n",
       "       149.690002, 148.679993, 149.330002, 149.360001, 149.820007,\n",
       "       147.220001, 148.990005, 148.660004, 150.389999, 151.580002,\n",
       "       151.889999, 151.410004, 150.199997, 150.020004, 148.960007,\n",
       "       148.429993, 150.369995, 149.940002, 151.      , 153.710007,\n",
       "       157.649994, 161.679993, 161.119995, 160.75    , 159.570007,\n",
       "       159.369995, 159.990005, 167.479996, 158.740005, 164.020004,\n",
       "       164.289993, 169.080002, 172.130005, 174.910004, 175.210007,\n",
       "       181.119995, 175.25    , 175.110001, 179.279999, 169.929993,\n",
       "       168.279999, 171.559998, 173.039993, 175.850006, 177.089996,\n",
       "       180.160004, 179.330002, 179.470001, 178.089996, 177.830002,\n",
       "       182.630005, 179.610001, 172.699997, 172.889999, 169.080002,\n",
       "       172.320007, 176.119995, 175.779999, 171.339996, 171.509995,\n",
       "       170.      , 166.979996, 164.419998, 160.020004, 158.979996,\n",
       "       163.5     , 162.449997, 165.710007, 170.160004, 174.009995,\n",
       "       174.75    , 174.479996, 171.679993, 172.860001, 171.729996,\n",
       "       176.050003, 174.139999, 172.330002, 167.369995, 170.970001,\n",
       "       171.850006, 171.029999, 169.820007, 164.979996, 165.539993,\n",
       "       152.580002, 163.839996, 163.059998, 164.699997, 164.389999,\n",
       "       168.470001, 164.490005, 163.360001, 158.820007, 161.479996,\n",
       "       160.199997, 158.929993, 151.449997, 150.899994, 157.050003,\n",
       "       158.610001, 160.509995, 163.509995, 165.509995, 167.990005,\n",
       "       171.059998, 173.880005, 172.169998, 176.690002, 178.550003,\n",
       "       177.839996, 174.029999, 174.570007, 177.5     , 172.360001,\n",
       "       171.160004, 171.779999, 168.710007, 168.020004, 167.389999,\n",
       "       170.619995, 163.919998, 165.020004, 168.759995, 168.910004,\n",
       "       166.460007, 161.119995, 162.25    , 155.910004, 159.25    ,\n",
       "       161.839996, 156.710007, 158.149994, 159.669998, 163.850006,\n",
       "       156.009995, 154.929993, 155.520004, 153.5     , 142.770004,\n",
       "       144.589996, 145.550003, 148.860001, 146.850006, 139.880005,\n",
       "       139.089996, 137.789993, 140.809998, 138.429993, 137.389999,\n",
       "       145.389999, 149.070007, 149.899994, 147.830002, 146.899994,\n",
       "       147.029999, 144.350006, 148.580002, 147.080002, 140.279999,\n",
       "       132.869995, 133.130005, 134.289993, 132.080002, 130.070007,\n",
       "       133.419998, 134.789993, 136.820007, 139.899994, 142.699997,\n",
       "       142.130005, 137.460007, 137.25    , 136.039993, 137.770004,\n",
       "       141.350006, 143.289993, 145.259995, 145.669998, 145.759995,\n",
       "       142.990005, 144.080002, 149.779999, 150.740005, 147.919998,\n",
       "       151.119995, 154.5     , 155.389999, 154.009995, 152.259995,\n",
       "       152.580002, 156.979996, 161.240005, 161.009995, 160.100006,\n",
       "       160.839996, 166.009995, 163.210007, 166.369995, 164.020004,\n",
       "       167.679993, 170.059998, 169.820007, 171.520004, 172.779999,\n",
       "       172.770004, 173.75    , 173.029999, 169.690002, 167.080002,\n",
       "       167.320007, 168.779999, 170.570007, 161.149994, 162.130005,\n",
       "       160.309998, 156.639999, 159.75    , 156.470001, 154.820007,\n",
       "       154.639999, 155.470001, 159.589996, 159.899994, 154.789993,\n",
       "       154.649994, 151.210007, 149.309998, 153.399994, 157.339996,\n",
       "       152.380005, 151.190002, 149.660004, 152.740005, 147.639999,\n",
       "       146.100006, 141.279999, 138.210007, 145.029999, 144.070007,\n",
       "       145.809998, 142.539993, 140.419998, 139.899994, 139.130005,\n",
       "       134.990005, 144.309998, 141.070007, 145.490005, 141.690002,\n",
       "       143.020004, 142.869995, 147.190002, 150.089996, 150.960007,\n",
       "       148.070007, 148.199997, 153.160004, 155.080002, 148.949997,\n",
       "       142.059998, 142.089996, 137.110001, 140.410004, 138.5     ,\n",
       "       141.240005, 145.820007, 148.970001, 152.220001, 149.130005,\n",
       "       146.429993, 152.309998, 150.160004, 148.130005, 149.449997,\n",
       "       148.309998, 145.139999, 144.289993, 141.399994, 148.210007,\n",
       "       145.960007, 147.770004, 147.070007, 142.190002, 142.360001,\n",
       "       142.339996, 142.699997, 149.5     , 145.350006, 141.110001,\n",
       "       136.690002, 135.110001, 131.389999, 132.979996, 134.350006,\n",
       "       130.919998, 131.380005, 129.669998, 127.989998, 128.410004,\n",
       "       130.279999, 126.889999, 127.129997, 126.010002, 130.470001,\n",
       "       130.259995, 131.25    , 133.880005, 132.029999, 134.830002,\n",
       "       136.820007, 134.080002, 135.279999, 138.119995, 140.309998,\n",
       "       140.889999, 143.169998, 143.160004, 144.960007, 142.699997,\n",
       "       143.970001, 148.899994, 148.029999, 152.570007, 150.639999,\n",
       "       153.880005, 153.779999, 149.460007, 150.949997, 152.119995,\n",
       "       153.110001, 153.509995, 152.350006, 150.199997, 148.869995,\n",
       "       150.089996, 147.110001, 147.710007, 147.050003, 146.830002,\n",
       "       144.380005, 148.039993, 153.789993, 153.699997, 152.809998,\n",
       "       153.559998, 150.210007, 147.809998, 151.279999, 151.190002,\n",
       "       152.160004, 156.080002, 155.070007, 157.320007, 159.300003,\n",
       "       158.830002, 158.860001, 159.940002, 157.970001, 159.369995,\n",
       "       161.529999, 162.440002, 164.270004, 166.600006, 164.740005,\n",
       "       162.429993, 161.419998, 162.350006, 161.220001, 161.630005,\n",
       "       164.589996, 165.089996, 166.100006, 165.800003, 166.089996,\n",
       "       165.050003, 165.      , 165.190002, 163.059998, 165.190002,\n",
       "       168.490005, 169.279999, 170.089996, 169.5     , 164.889999,\n",
       "       170.979996, 172.479996, 173.050003, 173.020004, 173.850006,\n",
       "       173.619995, 173.160004, 171.990005, 171.710007, 173.      ,\n",
       "       176.389999, 173.979996, 173.130005, 171.089996, 172.410004,\n",
       "       173.320007, 176.960007, 177.330002, 177.699997, 181.029999,\n",
       "       182.630005, 179.970001, 178.440002, 177.899994, 181.5     ,\n",
       "       181.270004, 182.800003, 183.369995, 183.960007, 186.729996,\n",
       "       184.410004, 184.899994, 183.740005, 185.550003, 186.830002,\n",
       "       185.889999, 187.929993, 189.080002, 191.630005])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cecfd7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred Values--  [142.32146  143.60548  144.743    145.34113  145.83081  143.9273\n",
      " 142.56401  143.527    144.76323  145.46164  144.86298  144.142\n",
      " 144.08124  146.51134  147.36426  148.1469   148.70567  149.12102\n",
      " 150.55698  150.35443  149.69691  146.55792  145.38724  144.35588\n",
      " 144.14029  144.2394   143.02243  140.1134   139.97935  141.2283\n",
      " 142.5579   142.87999  141.87367  139.7019   139.50871  138.99532\n",
      " 138.54881  137.10544  137.42952  137.69073  139.55038  139.92537\n",
      " 139.71733  138.95813  137.83249  139.26256  140.64336  141.62193\n",
      " 143.74255  144.85747  145.02245  145.00201  144.5839   145.15625\n",
      " 144.92987  146.66003  144.90375  144.69759  145.33438  146.42348\n",
      " 146.76517  146.8216   146.46614  146.37015  144.82925  144.32736\n",
      " 144.93846  145.84154  146.2715   147.91039  150.61484  153.28223\n",
      " 155.50186  155.00201  155.15906  152.47342  153.62004  156.18927\n",
      " 158.57397  155.78264  155.59831  157.96434  161.9712   165.07564\n",
      " 166.11797  168.27844  167.92833  165.58952  167.30804  165.25964\n",
      " 162.60385  161.28749  163.26088  165.55109  167.10336  169.5032\n",
      " 169.95738  169.71704  169.2256   168.4834   170.37944  170.46136\n",
      " 167.25398  164.2063   163.82594  162.80028  164.96466  166.76437\n",
      " 165.04771  164.27654  162.62877  160.22414  158.61002  156.73106\n",
      " 153.91914  153.47351  154.16403  154.04572  159.59602  164.41869\n",
      " 165.7436   166.4769   165.14973  164.00629  163.7652   165.02573\n",
      " 166.93796  164.6727   162.0363   160.74734  163.514    163.94904\n",
      " 162.17043  160.634    157.80466  155.54692  153.15915  156.66853\n",
      " 157.62662  157.32677  158.6102   159.70009  157.34032  154.8733\n",
      " 152.54144  155.13818  153.05391  150.9333   147.19437  148.2486\n",
      " 151.87372  153.8564   156.13551  158.05823  160.31891  162.07542\n",
      " 164.455    165.73021  165.78635  168.48463  168.56813  166.62617\n",
      " 165.1691   167.48784  166.86559  163.8007   163.4517   162.53094\n",
      " 159.70955  160.27882  161.55428  159.9852   158.28922  159.22429\n",
      " 160.20345  160.27063  157.19887  155.50407  152.95544  151.44035\n",
      " 155.08743  153.87614  151.61478  152.80338  156.59363  153.28229\n",
      " 151.48633  148.4621   149.17223  145.07439  139.74107  141.40977\n",
      " 141.77942  144.13206  140.2197   136.283    134.5418   137.08745\n",
      " 137.1279   137.14958  138.21196  143.1447   144.66547  145.17854\n",
      " 145.60239  142.80327  142.53537  143.06007  144.10799  141.31638\n",
      " 136.50696  131.49036  130.47958  131.80211  129.39955  129.16336\n",
      " 131.9773   132.99304  134.68915  137.50629  138.82518  136.92905\n",
      " 136.08882  134.24982  134.84799  136.59297  138.85226  141.35997\n",
      " 142.71912  141.91473  142.33995  141.33478  142.62984  145.10873\n",
      " 144.44542  145.26796  147.38483  149.34398  149.66203  148.81915\n",
      " 147.5178   149.70892  151.20761  155.14381  155.75188  154.79141\n",
      " 157.34282  158.9075   158.25388  158.6514   158.05971  160.8628\n",
      " 161.6723   163.28242  164.54721  164.77036  165.65898  165.66791\n",
      " 164.13281  161.0373   160.15892  160.12984  161.84605  159.1824\n",
      " 155.60022  153.94768  152.66635  151.8279   151.58961  150.27426\n",
      " 150.33733  149.70798  151.23189  155.46466  151.34521  150.24402\n",
      " 148.45476  146.12636  147.5306   150.17044  150.13637  148.33965\n",
      " 146.31792  146.37642  147.21912  144.81966  140.67163  137.07788\n",
      " 137.37645  140.9942   141.91606  142.36961  138.83269  137.56804\n",
      " 136.68025  136.14333  136.83179  137.20006  138.42862  140.01271\n",
      " 140.13683  140.39569  141.84312  144.16287  146.69435  145.87946\n",
      " 142.73409  147.13039  148.34663  147.36917  143.55678  138.0467\n",
      " 135.7745   135.12761  136.17116  134.14426  139.39351  143.52458\n",
      " 144.80687  146.25984  145.15042  145.18105  146.70683  145.07303\n",
      " 145.11037  146.173    144.67953  141.80075  139.32211  141.20004\n",
      " 143.7277   143.65337  143.69185  141.25655  138.81909  139.08122\n",
      " 139.26132  140.06502  142.28314  140.77281  136.37373  133.25414\n",
      " 131.22237  129.98872  131.77368  130.87138  129.87675  128.79678\n",
      " 126.555275 127.12691  127.46326  125.697014 125.174034 124.544556\n",
      " 125.978516 128.27502  128.58655  130.17506  131.04091  131.62889\n",
      " 133.03127  133.58682  133.00958  133.96451  136.76042  138.57645\n",
      " 138.42307  139.92374  141.41402  140.70174  140.4291   140.89944\n",
      " 144.87732  147.51582  147.68675  148.53625  148.08005  147.37444\n",
      " 146.39128  147.96758  148.1691   149.55884  149.5059   148.1417\n",
      " 145.6854   144.84668  145.06711  143.4187   144.10889  143.97891\n",
      " 142.71452  142.16869  145.15698  148.97217  148.2876   148.4276\n",
      " 147.50667  145.37206  145.63358  147.26434  147.69347  149.49619\n",
      " 150.33423  151.2873   152.87424  153.32619  153.57135  153.93845\n",
      " 153.47638  152.38289  154.30237  155.94525  157.51491  158.9341\n",
      " 159.3319   157.6369   157.5078   155.84872  155.37724  154.82433\n",
      " 157.38641  158.56367  158.68037  159.67534  160.23193  159.9244\n",
      " 158.8069   158.55162  158.047    157.52296  160.0555   161.86882\n",
      " 162.30948  161.7673   161.14482  159.06076  164.01889  165.14345\n",
      " 164.5208   165.07242  165.38048  164.71555  164.42159  164.35904\n",
      " 164.247    165.85995  166.98863  166.05923  164.34991  163.82701\n",
      " 164.7032   166.2704   168.46884  168.60603  169.76007  171.05873\n",
      " 170.69154  169.66202  169.08191  169.99927  171.37204  172.70012\n",
      " 173.07619  173.24759  174.57796  174.53113  174.34442  173.55052\n",
      " 174.87888  175.38652  175.10968  176.08041  177.31825  177.86972 ]\n",
      "\n",
      "Original Values--  [146.190002 148.970001 148.539993 150.229996 149.800003 145.029999\n",
      " 147.440002 148.309998 149.449997 149.809998 148.350006 147.479996\n",
      " 149.       152.660004 152.830002 153.869995 153.759995 154.970001\n",
      " 156.979996 155.490005 155.       150.630005 150.350006 148.559998\n",
      " 148.440002 148.820007 143.800003 143.929993 144.449997 146.649994\n",
      " 145.660004 145.470001 143.25     142.470001 143.660004 141.899994\n",
      " 141.759995 139.490005 139.470001 143.059998 144.029999 142.270004\n",
      " 143.229996 141.240005 142.110001 143.770004 143.449997 147.009995\n",
      " 148.699997 148.809998 149.690002 148.679993 149.330002 149.360001\n",
      " 149.820007 147.220001 148.990005 148.660004 150.389999 151.580002\n",
      " 151.889999 151.410004 150.199997 150.020004 148.960007 148.429993\n",
      " 150.369995 149.940002 151.       153.710007 157.649994 161.679993\n",
      " 161.119995 160.75     159.570007 159.369995 159.990005 167.479996\n",
      " 158.740005 164.020004 164.289993 169.080002 172.130005 174.910004\n",
      " 175.210007 181.119995 175.25     175.110001 179.279999 169.929993\n",
      " 168.279999 171.559998 173.039993 175.850006 177.089996 180.160004\n",
      " 179.330002 179.470001 178.089996 177.830002 182.630005 179.610001\n",
      " 172.699997 172.889999 169.080002 172.320007 176.119995 175.779999\n",
      " 171.339996 171.509995 170.       166.979996 164.419998 160.020004\n",
      " 158.979996 163.5      162.449997 165.710007 170.160004 174.009995\n",
      " 174.75     174.479996 171.679993 172.860001 171.729996 176.050003\n",
      " 174.139999 172.330002 167.369995 170.970001 171.850006 171.029999\n",
      " 169.820007 164.979996 165.539993 152.580002 163.839996 163.059998\n",
      " 164.699997 164.389999 168.470001 164.490005 163.360001 158.820007\n",
      " 161.479996 160.199997 158.929993 151.449997 150.899994 157.050003\n",
      " 158.610001 160.509995 163.509995 165.509995 167.990005 171.059998\n",
      " 173.880005 172.169998 176.690002 178.550003 177.839996 174.029999\n",
      " 174.570007 177.5      172.360001 171.160004 171.779999 168.710007\n",
      " 168.020004 167.389999 170.619995 163.919998 165.020004 168.759995\n",
      " 168.910004 166.460007 161.119995 162.25     155.910004 159.25\n",
      " 161.839996 156.710007 158.149994 159.669998 163.850006 156.009995\n",
      " 154.929993 155.520004 153.5      142.770004 144.589996 145.550003\n",
      " 148.860001 146.850006 139.880005 139.089996 137.789993 140.809998\n",
      " 138.429993 137.389999 145.389999 149.070007 149.899994 147.830002\n",
      " 146.899994 147.029999 144.350006 148.580002 147.080002 140.279999\n",
      " 132.869995 133.130005 134.289993 132.080002 130.070007 133.419998\n",
      " 134.789993 136.820007 139.899994 142.699997 142.130005 137.460007\n",
      " 137.25     136.039993 137.770004 141.350006 143.289993 145.259995\n",
      " 145.669998 145.759995 142.990005 144.080002 149.779999 150.740005\n",
      " 147.919998 151.119995 154.5      155.389999 154.009995 152.259995\n",
      " 152.580002 156.979996 161.240005 161.009995 160.100006 160.839996\n",
      " 166.009995 163.210007 166.369995 164.020004 167.679993 170.059998\n",
      " 169.820007 171.520004 172.779999 172.770004 173.75     173.029999\n",
      " 169.690002 167.080002 167.320007 168.779999 170.570007 161.149994\n",
      " 162.130005 160.309998 156.639999 159.75     156.470001 154.820007\n",
      " 154.639999 155.470001 159.589996 159.899994 154.789993 154.649994\n",
      " 151.210007 149.309998 153.399994 157.339996 152.380005 151.190002\n",
      " 149.660004 152.740005 147.639999 146.100006 141.279999 138.210007\n",
      " 145.029999 144.070007 145.809998 142.539993 140.419998 139.899994\n",
      " 139.130005 134.990005 144.309998 141.070007 145.490005 141.690002\n",
      " 143.020004 142.869995 147.190002 150.089996 150.960007 148.070007\n",
      " 148.199997 153.160004 155.080002 148.949997 142.059998 142.089996\n",
      " 137.110001 140.410004 138.5      141.240005 145.820007 148.970001\n",
      " 152.220001 149.130005 146.429993 152.309998 150.160004 148.130005\n",
      " 149.449997 148.309998 145.139999 144.289993 141.399994 148.210007\n",
      " 145.960007 147.770004 147.070007 142.190002 142.360001 142.339996\n",
      " 142.699997 149.5      145.350006 141.110001 136.690002 135.110001\n",
      " 131.389999 132.979996 134.350006 130.919998 131.380005 129.669998\n",
      " 127.989998 128.410004 130.279999 126.889999 127.129997 126.010002\n",
      " 130.470001 130.259995 131.25     133.880005 132.029999 134.830002\n",
      " 136.820007 134.080002 135.279999 138.119995 140.309998 140.889999\n",
      " 143.169998 143.160004 144.960007 142.699997 143.970001 148.899994\n",
      " 148.029999 152.570007 150.639999 153.880005 153.779999 149.460007\n",
      " 150.949997 152.119995 153.110001 153.509995 152.350006 150.199997\n",
      " 148.869995 150.089996 147.110001 147.710007 147.050003 146.830002\n",
      " 144.380005 148.039993 153.789993 153.699997 152.809998 153.559998\n",
      " 150.210007 147.809998 151.279999 151.190002 152.160004 156.080002\n",
      " 155.070007 157.320007 159.300003 158.830002 158.860001 159.940002\n",
      " 157.970001 159.369995 161.529999 162.440002 164.270004 166.600006\n",
      " 164.740005 162.429993 161.419998 162.350006 161.220001 161.630005\n",
      " 164.589996 165.089996 166.100006 165.800003 166.089996 165.050003\n",
      " 165.       165.190002 163.059998 165.190002 168.490005 169.279999\n",
      " 170.089996 169.5      164.889999 170.979996 172.479996 173.050003\n",
      " 173.020004 173.850006 173.619995 173.160004 171.990005 171.710007\n",
      " 173.       176.389999 173.979996 173.130005 171.089996 172.410004\n",
      " 173.320007 176.960007 177.330002 177.699997 181.029999 182.630005\n",
      " 179.970001 178.440002 177.899994 181.5      181.270004 182.800003\n",
      " 183.369995 183.960007 186.729996 184.410004 184.899994 183.740005\n",
      " 185.550003 186.830002 185.889999 187.929993 189.080002 191.630005]\n"
     ]
    }
   ],
   "source": [
    "print(\"Pred Values-- \" ,pred)\n",
    "print(\"\\nOriginal Values-- \",original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1986d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07e37414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc371be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dates = df.index[-len(original):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bfa9b94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(df_dates, original, color = 'red', label = 'Real Stock Price')\n",
    "plt.plot(df_dates, pred, color = 'blue', label = 'Predicted  Stock Price')\n",
    "plt.title(' Stock Price Prediction')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel(' Stock Price')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ddb80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a095e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30_days_past=df.iloc[-30:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "451dafea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-08-18</th>\n",
       "      <td>101.080002</td>\n",
       "      <td>103.470001</td>\n",
       "      <td>100.760002</td>\n",
       "      <td>101.410004</td>\n",
       "      <td>101.242561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-19</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.820000</td>\n",
       "      <td>98.599998</td>\n",
       "      <td>99.419998</td>\n",
       "      <td>99.255844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-20</th>\n",
       "      <td>99.589996</td>\n",
       "      <td>100.669998</td>\n",
       "      <td>99.099998</td>\n",
       "      <td>100.050003</td>\n",
       "      <td>99.884804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-23</th>\n",
       "      <td>100.599998</td>\n",
       "      <td>101.480003</td>\n",
       "      <td>100.269997</td>\n",
       "      <td>100.970001</td>\n",
       "      <td>100.803284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-24</th>\n",
       "      <td>101.290001</td>\n",
       "      <td>103.510002</td>\n",
       "      <td>101.059998</td>\n",
       "      <td>103.269997</td>\n",
       "      <td>103.099487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-25</th>\n",
       "      <td>103.360001</td>\n",
       "      <td>105.129997</td>\n",
       "      <td>102.550003</td>\n",
       "      <td>104.699997</td>\n",
       "      <td>104.527122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-26</th>\n",
       "      <td>104.459999</td>\n",
       "      <td>104.620003</td>\n",
       "      <td>102.839996</td>\n",
       "      <td>103.379997</td>\n",
       "      <td>103.209305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-27</th>\n",
       "      <td>103.900002</td>\n",
       "      <td>106.150002</td>\n",
       "      <td>103.900002</td>\n",
       "      <td>106.089996</td>\n",
       "      <td>105.914825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-30</th>\n",
       "      <td>106.330002</td>\n",
       "      <td>106.459999</td>\n",
       "      <td>104.800003</td>\n",
       "      <td>105.190002</td>\n",
       "      <td>105.016312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-31</th>\n",
       "      <td>104.940002</td>\n",
       "      <td>106.360001</td>\n",
       "      <td>104.459999</td>\n",
       "      <td>105.410004</td>\n",
       "      <td>105.235954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01</th>\n",
       "      <td>105.360001</td>\n",
       "      <td>105.470001</td>\n",
       "      <td>103.260002</td>\n",
       "      <td>103.660004</td>\n",
       "      <td>103.488846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-02</th>\n",
       "      <td>104.000000</td>\n",
       "      <td>106.339996</td>\n",
       "      <td>103.980003</td>\n",
       "      <td>106.260002</td>\n",
       "      <td>106.084557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-03</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>107.360001</td>\n",
       "      <td>104.080002</td>\n",
       "      <td>104.750000</td>\n",
       "      <td>104.577042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-07</th>\n",
       "      <td>104.500000</td>\n",
       "      <td>104.940002</td>\n",
       "      <td>102.309998</td>\n",
       "      <td>103.290001</td>\n",
       "      <td>103.119453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-08</th>\n",
       "      <td>102.949997</td>\n",
       "      <td>104.730003</td>\n",
       "      <td>100.419998</td>\n",
       "      <td>102.580002</td>\n",
       "      <td>102.410629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-09</th>\n",
       "      <td>102.360001</td>\n",
       "      <td>104.919998</td>\n",
       "      <td>102.110001</td>\n",
       "      <td>103.290001</td>\n",
       "      <td>103.119453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-10</th>\n",
       "      <td>103.290001</td>\n",
       "      <td>103.949997</td>\n",
       "      <td>101.779999</td>\n",
       "      <td>102.000000</td>\n",
       "      <td>101.831581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-13</th>\n",
       "      <td>102.650002</td>\n",
       "      <td>104.860001</td>\n",
       "      <td>102.019997</td>\n",
       "      <td>104.459999</td>\n",
       "      <td>104.287521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-14</th>\n",
       "      <td>104.540001</td>\n",
       "      <td>104.550003</td>\n",
       "      <td>100.070000</td>\n",
       "      <td>100.379997</td>\n",
       "      <td>100.214256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-15</th>\n",
       "      <td>100.790001</td>\n",
       "      <td>102.470001</td>\n",
       "      <td>100.690002</td>\n",
       "      <td>102.160004</td>\n",
       "      <td>101.991325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-16</th>\n",
       "      <td>102.389999</td>\n",
       "      <td>102.699997</td>\n",
       "      <td>100.820000</td>\n",
       "      <td>101.339996</td>\n",
       "      <td>101.172668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-17</th>\n",
       "      <td>101.150002</td>\n",
       "      <td>101.860001</td>\n",
       "      <td>100.129997</td>\n",
       "      <td>100.470001</td>\n",
       "      <td>100.304115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-20</th>\n",
       "      <td>98.500000</td>\n",
       "      <td>99.870003</td>\n",
       "      <td>97.269997</td>\n",
       "      <td>99.809998</td>\n",
       "      <td>99.645195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-21</th>\n",
       "      <td>99.900002</td>\n",
       "      <td>99.989998</td>\n",
       "      <td>96.209999</td>\n",
       "      <td>96.820000</td>\n",
       "      <td>96.660133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-22</th>\n",
       "      <td>97.669998</td>\n",
       "      <td>99.029999</td>\n",
       "      <td>97.660004</td>\n",
       "      <td>98.540001</td>\n",
       "      <td>98.377296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-23</th>\n",
       "      <td>99.529999</td>\n",
       "      <td>104.080002</td>\n",
       "      <td>99.519997</td>\n",
       "      <td>102.959999</td>\n",
       "      <td>102.789993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-24</th>\n",
       "      <td>102.660004</td>\n",
       "      <td>104.199997</td>\n",
       "      <td>102.599998</td>\n",
       "      <td>103.800003</td>\n",
       "      <td>103.709198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-27</th>\n",
       "      <td>104.550003</td>\n",
       "      <td>106.330002</td>\n",
       "      <td>104.389999</td>\n",
       "      <td>105.349998</td>\n",
       "      <td>105.257835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-28</th>\n",
       "      <td>105.290001</td>\n",
       "      <td>106.750000</td>\n",
       "      <td>104.730003</td>\n",
       "      <td>105.730003</td>\n",
       "      <td>105.637512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-29</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>105.309998</td>\n",
       "      <td>106.279999</td>\n",
       "      <td>106.187027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close\n",
       "Date                                                                  \n",
       "2021-08-18  101.080002  103.470001  100.760002  101.410004  101.242561\n",
       "2021-08-19  100.000000  100.820000   98.599998   99.419998   99.255844\n",
       "2021-08-20   99.589996  100.669998   99.099998  100.050003   99.884804\n",
       "2021-08-23  100.599998  101.480003  100.269997  100.970001  100.803284\n",
       "2021-08-24  101.290001  103.510002  101.059998  103.269997  103.099487\n",
       "2021-08-25  103.360001  105.129997  102.550003  104.699997  104.527122\n",
       "2021-08-26  104.459999  104.620003  102.839996  103.379997  103.209305\n",
       "2021-08-27  103.900002  106.150002  103.900002  106.089996  105.914825\n",
       "2021-08-30  106.330002  106.459999  104.800003  105.190002  105.016312\n",
       "2021-08-31  104.940002  106.360001  104.459999  105.410004  105.235954\n",
       "2021-09-01  105.360001  105.470001  103.260002  103.660004  103.488846\n",
       "2021-09-02  104.000000  106.339996  103.980003  106.260002  106.084557\n",
       "2021-09-03  106.000000  107.360001  104.080002  104.750000  104.577042\n",
       "2021-09-07  104.500000  104.940002  102.309998  103.290001  103.119453\n",
       "2021-09-08  102.949997  104.730003  100.419998  102.580002  102.410629\n",
       "2021-09-09  102.360001  104.919998  102.110001  103.290001  103.119453\n",
       "2021-09-10  103.290001  103.949997  101.779999  102.000000  101.831581\n",
       "2021-09-13  102.650002  104.860001  102.019997  104.459999  104.287521\n",
       "2021-09-14  104.540001  104.550003  100.070000  100.379997  100.214256\n",
       "2021-09-15  100.790001  102.470001  100.690002  102.160004  101.991325\n",
       "2021-09-16  102.389999  102.699997  100.820000  101.339996  101.172668\n",
       "2021-09-17  101.150002  101.860001  100.129997  100.470001  100.304115\n",
       "2021-09-20   98.500000   99.870003   97.269997   99.809998   99.645195\n",
       "2021-09-21   99.900002   99.989998   96.209999   96.820000   96.660133\n",
       "2021-09-22   97.669998   99.029999   97.660004   98.540001   98.377296\n",
       "2021-09-23   99.529999  104.080002   99.519997  102.959999  102.789993\n",
       "2021-09-24  102.660004  104.199997  102.599998  103.800003  103.709198\n",
       "2021-09-27  104.550003  106.330002  104.389999  105.349998  105.257835\n",
       "2021-09-28  105.290001  106.750000  104.730003  105.730003  105.637512\n",
       "2021-09-29  106.000000  107.000000  105.309998  106.279999  106.187027"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_30_days_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92497be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_30_days_future=pd.read_csv(\"test.csv\",parse_dates=[\"Date\"],index_col=[0])\n",
    "df_30_days_future.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7004371c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-09-30</th>\n",
       "      <td>107.089996</td>\n",
       "      <td>102.949997</td>\n",
       "      <td>103.029999</td>\n",
       "      <td>102.939865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-01</th>\n",
       "      <td>106.389999</td>\n",
       "      <td>103.669998</td>\n",
       "      <td>105.820000</td>\n",
       "      <td>105.727425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-04</th>\n",
       "      <td>107.080002</td>\n",
       "      <td>104.599998</td>\n",
       "      <td>104.900002</td>\n",
       "      <td>104.808235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-05</th>\n",
       "      <td>106.000000</td>\n",
       "      <td>103.750000</td>\n",
       "      <td>104.900002</td>\n",
       "      <td>104.808235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-06</th>\n",
       "      <td>104.419998</td>\n",
       "      <td>102.059998</td>\n",
       "      <td>104.330002</td>\n",
       "      <td>104.238731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-07</th>\n",
       "      <td>106.529999</td>\n",
       "      <td>104.330002</td>\n",
       "      <td>105.510002</td>\n",
       "      <td>105.417702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-08</th>\n",
       "      <td>106.220001</td>\n",
       "      <td>104.660004</td>\n",
       "      <td>104.720001</td>\n",
       "      <td>104.628387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-11</th>\n",
       "      <td>105.760002</td>\n",
       "      <td>103.970001</td>\n",
       "      <td>104.080002</td>\n",
       "      <td>103.988953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-12</th>\n",
       "      <td>104.040001</td>\n",
       "      <td>101.559998</td>\n",
       "      <td>102.720001</td>\n",
       "      <td>102.630142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-13</th>\n",
       "      <td>103.199997</td>\n",
       "      <td>101.180000</td>\n",
       "      <td>102.360001</td>\n",
       "      <td>102.270454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-14</th>\n",
       "      <td>103.650002</td>\n",
       "      <td>102.370003</td>\n",
       "      <td>102.739998</td>\n",
       "      <td>102.650116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-15</th>\n",
       "      <td>105.900002</td>\n",
       "      <td>103.190002</td>\n",
       "      <td>104.410004</td>\n",
       "      <td>104.318665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-18</th>\n",
       "      <td>104.570000</td>\n",
       "      <td>103.040001</td>\n",
       "      <td>104.120003</td>\n",
       "      <td>104.028915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-19</th>\n",
       "      <td>104.970001</td>\n",
       "      <td>103.580002</td>\n",
       "      <td>104.730003</td>\n",
       "      <td>104.638382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-20</th>\n",
       "      <td>106.019997</td>\n",
       "      <td>103.870003</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>105.907272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-21</th>\n",
       "      <td>106.389999</td>\n",
       "      <td>103.010002</td>\n",
       "      <td>103.150002</td>\n",
       "      <td>103.059761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-22</th>\n",
       "      <td>104.510002</td>\n",
       "      <td>102.550003</td>\n",
       "      <td>104.050003</td>\n",
       "      <td>103.958977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-25</th>\n",
       "      <td>105.989998</td>\n",
       "      <td>103.330002</td>\n",
       "      <td>105.300003</td>\n",
       "      <td>105.207886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-26</th>\n",
       "      <td>110.970001</td>\n",
       "      <td>105.220001</td>\n",
       "      <td>107.440002</td>\n",
       "      <td>107.346008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-27</th>\n",
       "      <td>108.279999</td>\n",
       "      <td>103.690002</td>\n",
       "      <td>103.849998</td>\n",
       "      <td>103.759148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-28</th>\n",
       "      <td>105.379997</td>\n",
       "      <td>103.099998</td>\n",
       "      <td>105.260002</td>\n",
       "      <td>105.167915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-29</th>\n",
       "      <td>105.239998</td>\n",
       "      <td>104.120003</td>\n",
       "      <td>104.870003</td>\n",
       "      <td>104.778259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-01</th>\n",
       "      <td>106.769997</td>\n",
       "      <td>105.279999</td>\n",
       "      <td>106.230003</td>\n",
       "      <td>106.137070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-02</th>\n",
       "      <td>107.139999</td>\n",
       "      <td>105.300003</td>\n",
       "      <td>106.690002</td>\n",
       "      <td>106.596664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-03</th>\n",
       "      <td>106.339996</td>\n",
       "      <td>104.820000</td>\n",
       "      <td>105.970001</td>\n",
       "      <td>105.877296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-04</th>\n",
       "      <td>106.400002</td>\n",
       "      <td>104.290001</td>\n",
       "      <td>105.209999</td>\n",
       "      <td>105.117958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-05</th>\n",
       "      <td>109.650002</td>\n",
       "      <td>106.849998</td>\n",
       "      <td>108.739998</td>\n",
       "      <td>108.644867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-08</th>\n",
       "      <td>110.309998</td>\n",
       "      <td>108.320000</td>\n",
       "      <td>108.419998</td>\n",
       "      <td>108.325150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-09</th>\n",
       "      <td>116.169998</td>\n",
       "      <td>110.480003</td>\n",
       "      <td>111.290001</td>\n",
       "      <td>111.192642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-10</th>\n",
       "      <td>112.680000</td>\n",
       "      <td>108.110001</td>\n",
       "      <td>108.959999</td>\n",
       "      <td>108.864677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  High         Low       Close   Adj Close\n",
       "Date                                                      \n",
       "2021-09-30  107.089996  102.949997  103.029999  102.939865\n",
       "2021-10-01  106.389999  103.669998  105.820000  105.727425\n",
       "2021-10-04  107.080002  104.599998  104.900002  104.808235\n",
       "2021-10-05  106.000000  103.750000  104.900002  104.808235\n",
       "2021-10-06  104.419998  102.059998  104.330002  104.238731\n",
       "2021-10-07  106.529999  104.330002  105.510002  105.417702\n",
       "2021-10-08  106.220001  104.660004  104.720001  104.628387\n",
       "2021-10-11  105.760002  103.970001  104.080002  103.988953\n",
       "2021-10-12  104.040001  101.559998  102.720001  102.630142\n",
       "2021-10-13  103.199997  101.180000  102.360001  102.270454\n",
       "2021-10-14  103.650002  102.370003  102.739998  102.650116\n",
       "2021-10-15  105.900002  103.190002  104.410004  104.318665\n",
       "2021-10-18  104.570000  103.040001  104.120003  104.028915\n",
       "2021-10-19  104.970001  103.580002  104.730003  104.638382\n",
       "2021-10-20  106.019997  103.870003  106.000000  105.907272\n",
       "2021-10-21  106.389999  103.010002  103.150002  103.059761\n",
       "2021-10-22  104.510002  102.550003  104.050003  103.958977\n",
       "2021-10-25  105.989998  103.330002  105.300003  105.207886\n",
       "2021-10-26  110.970001  105.220001  107.440002  107.346008\n",
       "2021-10-27  108.279999  103.690002  103.849998  103.759148\n",
       "2021-10-28  105.379997  103.099998  105.260002  105.167915\n",
       "2021-10-29  105.239998  104.120003  104.870003  104.778259\n",
       "2021-11-01  106.769997  105.279999  106.230003  106.137070\n",
       "2021-11-02  107.139999  105.300003  106.690002  106.596664\n",
       "2021-11-03  106.339996  104.820000  105.970001  105.877296\n",
       "2021-11-04  106.400002  104.290001  105.209999  105.117958\n",
       "2021-11-05  109.650002  106.849998  108.739998  108.644867\n",
       "2021-11-08  110.309998  108.320000  108.419998  108.325150\n",
       "2021-11-09  116.169998  110.480003  111.290001  111.192642\n",
       "2021-11-10  112.680000  108.110001  108.959999  108.864677"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_30_days_future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20277ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_30_days_future[\"Open\"]=0\n",
    "df_30_days_future=df_30_days_future[[\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"]]\n",
    "old_scaled_array=scaler.transform(df_30_days_past)\n",
    "new_scaled_array=scaler.transform(df_30_days_future)\n",
    "old_scaled_df=pd.DataFrame(old_scaled_array)\n",
    "new_scaled_df=pd.DataFrame(new_scaled_array)\n",
    "\n",
    "new_scaled_df.iloc[:,0]=np.nan\n",
    "full_df=pd.concat([old_scaled_df,new_scaled_df]).reset_index().drop([\"index\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0dcb1ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 5)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "88cfa224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.143640</td>\n",
       "      <td>0.164162</td>\n",
       "      <td>0.150135</td>\n",
       "      <td>0.349458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.152749</td>\n",
       "      <td>0.171268</td>\n",
       "      <td>0.159953</td>\n",
       "      <td>0.367565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.154599</td>\n",
       "      <td>0.175349</td>\n",
       "      <td>0.159063</td>\n",
       "      <td>0.365924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.171024</td>\n",
       "      <td>0.181345</td>\n",
       "      <td>0.167045</td>\n",
       "      <td>0.380645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.161242</td>\n",
       "      <td>0.174766</td>\n",
       "      <td>0.160565</td>\n",
       "      <td>0.368694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0         1         2         3         4\n",
       "55 NaN  0.143640  0.164162  0.150135  0.349458\n",
       "56 NaN  0.152749  0.171268  0.159953  0.367565\n",
       "57 NaN  0.154599  0.175349  0.159063  0.365924\n",
       "58 NaN  0.171024  0.181345  0.167045  0.380645\n",
       "59 NaN  0.161242  0.174766  0.160565  0.368694"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e8cc462a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48bf070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_scaled_array=full_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91fdcf82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 5)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_scaled_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "547622aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 8ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 1ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 5ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "all_data=[]\n",
    "time_step=30\n",
    "for i in range(time_step,len(full_df_scaled_array)):\n",
    "    data_x=[]\n",
    "    data_x.append(full_df_scaled_array[i-time_step:i,0:full_df_scaled_array.shape[1]])\n",
    "    data_x=np.array(data_x)\n",
    "    prediction=my_model.predict(data_x)\n",
    "    all_data.append(prediction)\n",
    "    full_df.iloc[i,0]=prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "05929d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.1482769]], dtype=float32),\n",
       " array([[0.15000442]], dtype=float32),\n",
       " array([[0.1514765]], dtype=float32),\n",
       " array([[0.1526272]], dtype=float32),\n",
       " array([[0.15333982]], dtype=float32),\n",
       " array([[0.15340161]], dtype=float32),\n",
       " array([[0.15370393]], dtype=float32),\n",
       " array([[0.15395224]], dtype=float32),\n",
       " array([[0.15393883]], dtype=float32),\n",
       " array([[0.15323266]], dtype=float32),\n",
       " array([[0.1521465]], dtype=float32),\n",
       " array([[0.15122248]], dtype=float32),\n",
       " array([[0.1509795]], dtype=float32),\n",
       " array([[0.15092765]], dtype=float32),\n",
       " array([[0.15114477]], dtype=float32),\n",
       " array([[0.15176119]], dtype=float32),\n",
       " array([[0.15193477]], dtype=float32),\n",
       " array([[0.15188926]], dtype=float32),\n",
       " array([[0.15212983]], dtype=float32),\n",
       " array([[0.15332372]], dtype=float32),\n",
       " array([[0.15389305]], dtype=float32),\n",
       " array([[0.15412788]], dtype=float32),\n",
       " array([[0.15420789]], dtype=float32),\n",
       " array([[0.15464287]], dtype=float32),\n",
       " array([[0.15528473]], dtype=float32),\n",
       " array([[0.15572071]], dtype=float32),\n",
       " array([[0.15582754]], dtype=float32),\n",
       " array([[0.1568256]], dtype=float32),\n",
       " array([[0.15823856]], dtype=float32),\n",
       " array([[0.16073725]], dtype=float32)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "379cf40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_array=np.array(all_data)\n",
    "#new_array=new_array.reshape(-1,1)\n",
    "prediction_copies_array = np.repeat(new_array, 5, axis=-1)\n",
    "y_pred_future_30_days = scaler.inverse_transform(np.reshape(prediction_copies_array,(len(new_array),5)))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4ab9307c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([104.84653 , 105.46312 , 105.98854 , 106.39925 , 106.653595,\n",
       "       106.67565 , 106.783554, 106.872185, 106.8674  , 106.61535 ,\n",
       "       106.22767 , 105.897865, 105.81114 , 105.79263 , 105.87013 ,\n",
       "       106.09014 , 106.15211 , 106.135864, 106.221725, 106.64785 ,\n",
       "       106.85106 , 106.93487 , 106.96343 , 107.11868 , 107.347786,\n",
       "       107.503395, 107.54152 , 107.89776 , 108.40207 , 109.29391 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_future_30_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265bfef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eed9c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ede34e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved!\n"
     ]
    }
   ],
   "source": [
    "my_model.save('Model_future_value.h5')\n",
    "print('Model Saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f12ddc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "13f766d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "scalerfile = 'scaler_model_future_value.pkl'\n",
    "pickle.dump(scaler, open(scalerfile, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbe5698",
   "metadata": {},
   "source": [
    "# END!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9749107",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
